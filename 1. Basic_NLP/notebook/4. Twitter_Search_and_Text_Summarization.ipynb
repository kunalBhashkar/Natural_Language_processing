{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Screen name = \"GahDamnNick\" Tweet = \"@KaiWasTakenX2 It‚Äôs people that‚Äôs taking advantage of the protest making the people that ACTUALLY want justice look bad\"\n",
      "2. Screen name = \"rileyfetter\" Tweet = \"https://t.co/hXjb63aeGi\"\n",
      "3. Screen name = \"TheTaylorPerrin\" Tweet = \"@Hok_keis Lmaoooo not ‚Äúso fuck her‚Äù üòÇüòÇ\"\n",
      "4. Screen name = \"brendafeaster5\" Tweet = \"@gtconway3d Section 230 needs some changes\"\n",
      "5. Screen name = \"ameer_ice_drake\" Tweet = \"Just posted a photo @ Atlanta, Georgia https://t.co/q8dRByhXLA\"\n",
      "6. Screen name = \"Abby_Spunky\" Tweet = \"Y‚Äôall Peachtree was a peachy af ü•∞\"\n",
      "7. Screen name = \"kailynnlee\" Tweet = \"This what I need to do I can‚Äôt skip into this dark space again\"\n",
      "8. Screen name = \"EmbraceMeHere\" Tweet = \"Someone said Zimmerman is just a poor batman and i don‚Äôt see the issue\"\n",
      "9. Screen name = \"mxrlonszn\" Tweet = \"With everything that‚Äôs going on I can‚Äôt even imagine what QAnon conspiracy theorists are cooking up\"\n",
      "10. Screen name = \"QueenNala___\" Tweet = \"I was just blasting I‚Äôd rather go blindü•¥\"\n",
      "11. Screen name = \"CountryGuy_01\" Tweet = \"Commie should be removed.\"\n",
      "12. Screen name = \"kailynnlee\" Tweet = \"This what I need to do I can‚Äôt slip into this dark space again\"\n",
      "13. Screen name = \"triplet2000\" Tweet = \"Unless you finna do this don‚Äôt call yourself an ally\"\n",
      "14. Screen name = \"turbocrob\" Tweet = \"When I was in Brazil, we visited a police station and when we talked to the kids thru language barriers, they expla‚Ä¶ https://t.co/r4q9kqBckZ\"\n",
      "15. Screen name = \"TheTaylorPerrin\" Tweet = \"That‚Äôs like me having a conversation w my home girl about bananas and a random person comin up and asking me why im‚Ä¶ https://t.co/ZqGAzlGCtO\"\n",
      "16. Screen name = \"ryuintokyo\" Tweet = \"@snapchatsupport Ok y‚Äôall just sent me a generic reply. No, WHY IS MY ACCOUNT LOCKED!\"\n",
      "17. Screen name = \"Asjiamonaee\" Tweet = \"Cute shit ü•∫ü•¥\"\n",
      "18. Screen name = \"AshleighJenaeH\" Tweet = \"actions hold more weight than words. #ShowMe üôÉüòâ\"\n",
      "19. Screen name = \"Guccijann\" Tweet = \"Every day I pray for your success.. Everyday I pray for you strength.. Even on our bad days you‚Äôre always present i‚Ä¶ https://t.co/SFCy37H6eu\"\n",
      "20. Screen name = \"CountryGuy_01\" Tweet = \"@KenGardner11 The commie mayor of Minneapolis should be removed and jailed right now.\"\n",
      "Max num reached = 20\n"
     ]
    }
   ],
   "source": [
    "from tweepy import (Stream, OAuthHandler)\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class Listener(StreamListener):\n",
    "\n",
    "    tweet_counter = 0 # Static variable\n",
    "    \n",
    "    def login(self):\n",
    "        CONSUMER_KEY = 'yoIwFkjZGYDa49aO16XqSNqcN'\n",
    "        CONSUMER_SECRET = 'gl4LQOItV7Z1aFwNrlvaiKJ3t8o8h99blMIAmnmdHxYjzjRAxO'\n",
    "        ACCESS_TOKEN = '624310916-E7fDF2IE8P6bfY1oVFglASf6F8RnxMd3vgSXFqnZ'\n",
    "        ACCESS_TOKEN_SECRET = 'ID9JcoXHsDcKtvNcnmBGcCQhUlO0wmwAxBJ6LCesiUAas'\n",
    "\n",
    "        auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        return auth\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        Listener.tweet_counter += 1\n",
    "        print(str(Listener.tweet_counter) + '. Screen name = \"%s\" Tweet = \"%s\"'\n",
    "              %(status.author.screen_name, status.text.replace('\\n', ' ')))\n",
    "\n",
    "        if Listener.tweet_counter < Listener.stop_at:\n",
    "            return True\n",
    "        else:\n",
    "            print('Max num reached = ' + str(Listener.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def getTweetsByGPS(self, stop_at_number, latitude_start, longitude_start, latitude_finish, longitude_finish):\n",
    "        try:\n",
    "            Listener.stop_at = stop_at_number # Create static variable\n",
    "            auth = self.login()\n",
    "            streaming_api = Stream(auth, Listener(), timeout=60) # Socket timeout value\n",
    "            streaming_api.filter(follow=None, locations=[latitude_start, longitude_start, latitude_finish, longitude_finish])\n",
    "        except KeyboardInterrupt:\n",
    "            print('Got keyboard interrupt')\n",
    "\n",
    "    def getTweetsByHashtag(self, stop_at_number, hashtag):\n",
    "        try:\n",
    "            Listener.stopAt = stop_at_number\n",
    "            auth = self.login()\n",
    "            streaming_api = Stream(auth, Listener(), timeout=60)\n",
    "            # Atlanta area.\n",
    "            streaming_api.filter(track=[hashtag])\n",
    "        except KeyboardInterrupt:\n",
    "            print('Got keyboard interrupt')\n",
    "\n",
    "listener = Listener()\n",
    "listener.getTweetsByGPS(20, -84.395198, 33.746876, -84.385585, 33.841601) # Atlanta area. Tool to find coordinates for any place: http://boundingbox.klokantech.com/ (use CSV as the output format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:     RT @Amila #Test\n",
      "Tom's newly listed Co. &amp; Mary's unlisted     Group to supply tech for nlTK.\n",
      "h.. $TSLA $AAPL https:// t.co/x34afsfQsh \n",
      "\n",
      "No special entitites:     RT @Amila #Test\n",
      "Tom's newly listed Co.  Mary's unlisted     Group to supply tech for nlTK.\n",
      "h.. $TSLA $AAPL https:// t.co/x34afsfQsh \n",
      "\n",
      "No tickers:     RT @Amila #Test\n",
      "Tom's newly listed Co.  Mary's unlisted     Group to supply tech for nlTK.\n",
      "h..   https:// t.co/x34afsfQsh \n",
      "\n",
      "No hyperlinks:     RT @Amila #Test\n",
      "Tom's newly listed Co.  Mary's unlisted     Group to supply tech for nlTK.\n",
      "h..    \n",
      "\n",
      "No hashtags:     RT @Amila \n",
      "Tom's newly listed Co.  Mary's unlisted     Group to supply tech for nlTK.\n",
      "h..    \n",
      "\n",
      "No punctuation:     RT @Amila \n",
      "Tom s newly listed Co   Mary s unlisted     Group to supply tech for nlTK \n",
      "h     \n",
      "\n",
      "No small words:      @Amila \n",
      "Tom  newly listed    Mary  unlisted     Group  supply tech for nlTK \n",
      "     \n",
      "\n",
      "No whitespace: @Amila Tom newly listed Mary unlisted Group supply tech for nlTK  \n",
      "\n",
      "No emojis: @Amila Tom newly listed Mary unlisted Group supply tech for nlTK  \n",
      "\n",
      "Tweet tokenize: ['tom', 'newly', 'listed', 'mary', 'unlisted', 'group', 'supply', 'tech', 'for', 'nltk'] \n",
      "\n",
      "No stop words: ['tom', 'newly', 'listed', 'mary', 'unlisted', 'group', 'supply', 'tech', 'nltk'] \n",
      "\n",
      "Final tweet: tom newly listed mary unlisted group supply tech nltk\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "punctuation += '¬¥ŒÑ‚Äô‚Ä¶‚Äú‚Äù‚Äì‚Äî‚Äï¬ª¬´' # string.punctuation misses these.\n",
    "\n",
    "cache_english_stopwords = stopwords.words('english') # Could speed up code by making this a set\n",
    "\n",
    "def tweet_clean(tweet):\n",
    "    print('Original tweet:', tweet, '\\n')\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet_no_special_entities = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    print('No special entitites:', tweet_no_special_entities, '\\n')\n",
    "    # Remove tickers (Clickable stock market symbols that work like hashtags and start with dollar signs instead)\n",
    "    tweet_no_tickers = re.sub(r'\\$\\w*', '', tweet_no_special_entities) # Substitute. $ needs to be escaped because it means something in regex. \\w means alphanumeric char or underscore.\n",
    "    print('No tickers:', tweet_no_tickers, '\\n')\n",
    "    # Remove hyperlinks\n",
    "    tweet_no_hyperlinks = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet_no_tickers)\n",
    "    print('No hyperlinks:', tweet_no_hyperlinks, '\\n')\n",
    "    # Remove hashtags\n",
    "    tweet_no_hashtags = re.sub(r'#\\w*', '', tweet_no_hyperlinks)\n",
    "    print('No hashtags:', tweet_no_hashtags, '\\n')\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet_no_punctuation = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet_no_hashtags)\n",
    "    print('No punctuation:', tweet_no_punctuation, '\\n')\n",
    "    # Remove words with 2 or fewer letters (Also takes care of RT)\n",
    "    tweet_no_small_words = re.sub(r'\\b\\w{1,2}\\b', '', tweet_no_punctuation) # \\b represents a word boundary\n",
    "    print('No small words:', tweet_no_small_words, '\\n')\n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet_no_whitespace = re.sub(r'\\s\\s+', ' ', tweet_no_small_words)\n",
    "    tweet_no_whitespace = tweet_no_whitespace.lstrip(' ') # Remove single space left on the left\n",
    "    print('No whitespace:', tweet_no_whitespace, '\\n')\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    tweet_no_emojis = ''.join(c for c in tweet_no_whitespace if c <= '\\uFFFF') # Apart from emojis (plane 1), this also removes historic scripts and mathematical alphanumerics (also plane 1), ideographs (plane 2) and more.\n",
    "    print('No emojis:', tweet_no_emojis, '\\n')\n",
    "    # Tokenize: Change to lowercase, reduce length and remove handles\n",
    "    tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True) # reduce_len changes, for example, waaaaaayyyy to waaayyy.\n",
    "    tw_list = tknzr.tokenize(tweet_no_emojis)\n",
    "    print('Tweet tokenize:', tw_list, '\\n')\n",
    "    # Remove stopwords\n",
    "    list_no_stopwords = [i for i in tw_list if i not in cache_english_stopwords]\n",
    "    print('No stop words:', list_no_stopwords, '\\n')\n",
    "    # Final filtered tweet\n",
    "    tweet_filtered =' '.join(list_no_stopwords) # ''.join() would join without spaces between words.\n",
    "    print('Final tweet:', tweet_filtered)\n",
    "\n",
    "s = '    RT @Amila #Test\\nTom\\'s newly listed Co. &amp; Mary\\'s unlisted     Group to supply tech for nlTK.\\nh.. $TSLA $AAPL https:// t.co/x34afsfQsh'\n",
    "tweet_clean(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "# Gettings the data source\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n",
    "\n",
    "# Parsing the data/ creating BeautifulSoup object\n",
    "soup = bs.BeautifulSoup(source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "clean_text = text.lower()\n",
    "clean_text = re.sub(r'\\W',' ',clean_text)\n",
    "clean_text = re.sub(r'\\d',' ',clean_text)\n",
    "clean_text = re.sub(r'\\s+',' ',clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "# Stopword list\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word counts \n",
    "word2count = {}\n",
    "for word in nltk.word_tokenize(clean_text):\n",
    "    if word not in stop_words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "# Converting counts to weights\n",
    "max_count = max(word2count.values())\n",
    "for key in word2count.keys():\n",
    "    word2count[key] = word2count[key]/max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product sentence scores    \n",
    "sent2score = {}\n",
    "for sentence in sentences:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word2count.keys():\n",
    "            if len(sentence.split(' ')) < 25:\n",
    "                if sentence not in sent2score.keys():\n",
    "                    sent2score[sentence] = word2count[word]\n",
    "                else:\n",
    "                    sent2score[sentence] += word2count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming.\n",
      "The long-term effects of global warming include further ice melt, ocean warming, sea level rise, and ocean acidification.\n",
      "Countries work together on climate change under the umbrella of the United Nations Framework Convention on Climate Change (UNFCCC), which has near-universal membership.\n",
      "Geoengineering or climate engineering is the deliberate large-scale modification of the climate to counteract climate change.\n",
      "Many arctic animals rely on sea ice, which has been disappearing in a warming Arctic.\n"
     ]
    }
   ],
   "source": [
    "# Gettings best 5 lines             \n",
    "import heapq\n",
    "best_sentences = heapq.nlargest(5, sent2score, key=sent2score.get)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "for sentence in best_sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
