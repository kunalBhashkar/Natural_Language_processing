{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `langdetect` and `langid` Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `langdetect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`langdetect`](https://pypi.org/project/langdetect/) supports 55 languages out of the box ([ISO 639-1 codes](https://www.wikiwand.com/en/List_of_ISO_639-1_codes)):\n",
    "\n",
    "> af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he,\n",
    "hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl,\n",
    "pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"You are a noob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Eisai enas ilithios re.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Είσαι\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[en:0.7142816943076757, id:0.28571612486335973]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_langs(\"What's up man?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `langid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`langid`](https://github.com/saffsd/langid.py) comes pre-trained on 97 languages (ISO 639-1 codes given):\n",
    "\n",
    "af, am, an, ar, as, az, be, bg, bn, br, bs, ca, cs, cy, da, de, dz, el, en, eo, es, et, eu, fa, fi, fo, fr, ga, gl, gu, he, hi, hr, ht, hu, hy, id, is, it, ja, jv, ka, kk, km, kn, ko, ku, ky, la, lb, lo, lt, lv, mg, mk, ml, mn, mr, ms, mt, nb, ne, nl, nn, no, oc, or, pa, pl, ps, pt, qu, ro, ru, rw, se, si, sk, sl, sq, sr, sv, sw, ta, te, th, tl, tr, ug, uk, ur, vi, vo, wa, xh, zh, zu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(\"You are a noob.\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(\"Eisai enas ilithios re.\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en', -25.157379627227783)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(\"What's up man?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en', 0.9999887931088783)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.classify(\"What's up man?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('de', -39.938232421875)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.identifier.set_languages(['de','fr','it'])\n",
    "langid.identifier.classify(\"What's up man?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg Selections http://gutenberg.net/  This corpus contains etexts from from Project Gutenberg, by the following authors:  * Jane Austen (3) * William Blake (2) * Thornton W. Burgess * Sarah Cone Bryant * Lewis Carroll * G. K. Chesterton (3) * Maria Edgeworth * King James Bible * Herman Melville * John Milton * William Shakespeare (3) * Walt Whitman  The beginning of the body of each book could not be identified automatically, so the semi-generic header of each file has been removed, and included below. Some source files ended with a line \"End of The Project Gutenberg Etext...\", and this has been deleted.  Information about Project Gutenberg (one page)  We produce about two million dollars for each hour we work.  The fifty hours is one conservative estimate for how long it we take to get any etext selected, entered, proofread, edited, copyright searched and analyzed, the copyright letters written, etc.  This projected audience is one hundred million readers.  If our value per text is nominally estimated at one dollar, then we produce 2 million dollars per hour this year we, will have to do four text files per month:  thus upping our productivity from one million. The Goal of Project Gutenberg is to Give Away One Trillion Etext Files by the December 31, 2001.  [10,000 x 100,000,000=Trillion] This is ten thousand titles each to one hundred million readers, which is 10% of the expected number of computer users by the end of the year 2001.  We need your donations more than ever!  All donations should be made to \"Project Gutenberg/IBC\", and are tax deductible to the extent allowable by law (\"IBC\" is Illinois Benedictine College).  (Subscriptions to our paper newsletter go to IBC, too)  For these and other matters, please mail to:  Project Gutenberg P. O. Box  2782 Champaign, IL 61825  When all other email fails try our Michael S. Hart, Executive Director: hart@vmd.cso.uiuc.edu (internet)   hart@uiucvmd   (bitnet)  We would prefer to send you this information by email (Internet, Bitnet, Compuserve, ATTMAIL or MCImail).  ****** If you have an FTP program (or emulator), please FTP directly to the Project Gutenberg archives: [Mac users, do NOT point and click. . .type]  ftp mrcnext.cso.uiuc.edu login:  anonymous password:  your@login cd etext/etext91 or cd etext92 or cd etext93 [for new books]  [now also in cd etext/etext93] or cd etext/articles [get suggest gut for more information] dir [to see files] get or mget [to get files. . .set bin for zip files] get INDEX100.GUT get INDEX200.GUT for a list of books and get NEW.GUT for general information and mget GUT* for newsletters.  **Information prepared by the Project Gutenberg legal advisor** (Three Pages)   ***START**THE SMALL PRINT!**FOR PUBLIC DOMAIN ETEXTS**START*** Why is this \"Small Print!\" statement here?  You know: lawyers. They tell us you might sue us if there is something wrong with your copy of this etext, even if you got it for free from someone other than us, and even if what\\'s wrong is not our fault.  So, among other things, this \"Small Print!\" statement disclaims most of our liability to you.  It also tells you how you can distribute copies of this etext if you want to.  *BEFORE!* YOU USE OR READ THIS ETEXT By using or reading any part of this PROJECT GUTENBERG-tm etext, you indicate that you understand, agree to and accept this \"Small Print!\" statement.  If you do not, you can receive a refund of the money (if any) you paid for this etext by sending a request within 30 days of receiving it to the person you got it from.  If you received this etext on a physical medium (such as a disk), you must return it with your request.  ABOUT PROJECT GUTENBERG-TM ETEXTS This PROJECT GUTENBERG-tm etext, like most PROJECT GUTENBERG- tm etexts, is a \"public domain\" work distributed by Professor Michael S. Hart through the Project Gutenberg Association at Illinois Benedictine College (the \"Project\").  Among other things, this means that no one owns a United States copyright on or for this work, so the Project (and you!) can copy and distribute it in the United States without permission and without paying copyright royalties.  Special rules, set forth below, apply if you wish to copy and distribute this etext under the Project\\'s \"PROJECT GUTENBERG\" trademark.  To create these etexts, the Project expends considerable efforts to identify, transcribe and proofread public domain works.  Despite these efforts, the Project\\'s etexts and any medium they may be on may contain \"Defects\".  Among other things, Defects may take the form of incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other etext medium, a computer virus, or computer codes that damage or cannot be read by your equipment.  LIMITED WARRANTY; DISCLAIMER OF DAMAGES But for the \"Right of Replacement or Refund\" described below, [1] the Project (and any other party you may receive this etext from as a PROJECT GUTENBERG-tm etext) disclaims all liability to you for damages, costs and expenses, including legal fees, and [2] YOU HAVE NO REMEDIES FOR NEGLIGENCE OR UNDER STRICT LIABILITY, OR FOR BREACH OF WARRANTY OR CONTRACT, INCLUDING BUT NOT LIMITED TO INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES, EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES.  If you discover a Defect in this etext within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending an explanatory note within that time to the person you received it from.  If you received it on a physical medium, you must return it with your note, and such person may choose to alternatively give you a replacement copy.  If you received it electronically, such person may choose to alternatively give you a second opportunity to receive it electronically.  THIS ETEXT IS OTHERWISE PROVIDED TO YOU \"AS-IS\".  NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, ARE MADE TO YOU AS TO THE ETEXT OR ANY MEDIUM IT MAY BE ON, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.  Some states do not allow disclaimers of implied warranties or the exclusion or limitation of consequential damages, so the above disclaimers and exclusions may not apply to you, and you may have other legal rights.  INDEMNITY You will indemnify and hold the Project, its directors, officers, members and agents harmless from all liability, cost and expense, including legal fees, that arise directly or indirectly from any of the following that you do or cause: [1] distribution of this etext, [2] alteration, modification, or addition to the etext, or [3] any Defect.  DISTRIBUTION UNDER \"PROJECT GUTENBERG-tm\" You may distribute copies of this etext electronically, or by disk, book or any other medium if you either delete this \"Small Print!\" and all other references to Project Gutenberg, or:  [1]  Only give exact copies of it.  Among other things, this      requires that you do not remove, alter or modify the      etext or this \"small print!\" statement.  You may however,      if you wish, distribute this etext in machine readable      binary, compressed, mark-up, or proprietary form,      including any form resulting from conversion by word pro-      cessing or hypertext software, but only so long as      *EITHER*:       [*]  The etext, when displayed, is clearly readable, and           does *not* contain characters other than those           intended by the author of the work, although tilde           (~), asterisk (*) and underline (_) characters may           be used to convey punctuation intended by the           author, and additional characters may be used to           indicate hypertext links; OR       [*]  The etext may be readily converted by the reader at           no expense into plain ASCII, EBCDIC or equivalent           form by the program that displays the etext (as is           the case, for instance, with most word processors);           OR       [*]  You provide, or agree to also provide on request at           no additional cost, fee or expense, a copy of the           etext in its original plain ASCII form (or in EBCDIC           or other equivalent proprietary form).  [2]  Honor the etext refund and replacement provisions of this      \"Small Print!\" statement.  [3]  Pay a trademark license fee to the Project of 20% of the      net profits you derive calculated using the method you      already use to calculate your applicable taxes.  If you      don\\'t derive profits, no royalty is due.  Royalties are      payable to \"Project Gutenberg Association / Illinois      Benedictine College\" within the 60 days following each      date you prepare (or were legally required to prepare)      your annual (or equivalent periodic) tax return.  WHAT IF YOU *WANT* TO SEND MONEY EVEN IF YOU DON\\'T HAVE TO? The Project gratefully accepts contributions in money, time, scanning machines, OCR software, public domain etexts, royalty free copyright licenses, and every other sort of contribution you can think of.  Money should be paid to \"Project Gutenberg Association / Illinois Benedictine College\".  This \"Small Print!\" by Charles B. Kramer, Attorney Internet (72600.2026@compuserve.com); TEL: (212-254-5093) *END*THE SMALL PRINT! FOR PUBLIC DOMAIN ETEXTS*Ver.04.29.93*END* '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30103"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_sents = gutenberg.sents('bible-kjv.txt')\n",
    "len(bible_kjv_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'god',\n",
       " 'created',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "discard_punctuation_and_lowercased_sents = [[word.lower() for word in sent if word not in punctuation and word.isalpha()] \n",
    "                                            for sent in bible_kjv_sents]\n",
    "discard_punctuation_and_lowercased_sents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bible_kjv_word2vec_model = Word2Vec(discard_punctuation_and_lowercased_sents, min_count=5, size=200)\n",
    "bible_kjv_word2vec_model.save('bible_word2vec_gensim')\n",
    "# model = Word2Vec.load(fname) # To load a model\n",
    "word_vectors = bible_kjv_word2vec_model.wv\n",
    "del bible_kjv_word2vec_model # When we finish training the model, we can only delete it and keep the word vectors.\n",
    "word_vectors.save_word2vec_format('bible_word2vec_org', 'bible_word2vec_vocabulary')\n",
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lord', 0.7784607410430908),\n",
       " ('truth', 0.7335808277130127),\n",
       " ('spirit', 0.7329315543174744),\n",
       " ('glory', 0.7322947978973389),\n",
       " ('salvation', 0.7318679094314575),\n",
       " ('faith', 0.7294133901596069),\n",
       " ('hosts', 0.72527015209198),\n",
       " ('gospel', 0.7025399804115295),\n",
       " ('christ', 0.6934913396835327),\n",
       " ('mercy', 0.6844473481178284)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(['god']) # Most similar as in closest in the word graph. Word2vec is essentially about proportions of word occurrences in relations holding in general over large corpora of text. Consider word analogy ‘man is to woman as king is to X’ which was famously demonstrated in word2vec. The algorithm is able to come up with an answer queen, almost magically by simple vector differences. The main idea, called distributional hypothesis, is that similar words appear in similar contexts of words around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('earth', 0.745545506477356),\n",
       " ('heavens', 0.7376387715339661),\n",
       " ('mountains', 0.6927394866943359)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(['heaven'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6369451284408569)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9865918159484863)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The `_cosmul` variant uses a slightly-different comparison when using multiple positive/negative examples (such as when asking about analogies). One paper has shown it does better:\n",
    "word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77846074"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('lord', 'god')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'food'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.doesnt_match(\"lord god salvation food spirit\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gensim - pip install gensim\n",
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gettings the data source\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n",
    "\n",
    "# Parsing the data/ creating BeautifulSoup object\n",
    "soup = bs.BeautifulSoup(source,'lxml')\n",
    "\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\W',' ',text)\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the Word2Vec model\n",
    "# model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding Word Vectors\n",
    "# vector = model.wv['global']\n",
    "\n",
    "# # Most similar words\n",
    "# similar = model.wv.most_similar('warming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['global']\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install gensim - pip install gensim\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# filename = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# model.wv.most_similar('king')\n",
    "\n",
    "# model.wv.most_similar(positive=['king','woman'], negative= ['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward propagation\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.552083608453316\n",
      "Cost after epoch 10: 2.5518306130784434\n",
      "Cost after epoch 20: 2.5515632554518595\n",
      "Cost after epoch 30: 2.551264041967188\n",
      "Cost after epoch 40: 2.550915048022628\n",
      "Cost after epoch 50: 2.5504968480420236\n",
      "Cost after epoch 60: 2.5499979894971774\n",
      "Cost after epoch 70: 2.5493900805846765\n",
      "Cost after epoch 80: 2.548646292722973\n",
      "Cost after epoch 90: 2.5477369244063857\n",
      "Cost after epoch 100: 2.546627772539722\n",
      "Cost after epoch 110: 2.5453059252950707\n",
      "Cost after epoch 120: 2.5437138973935265\n",
      "Cost after epoch 130: 2.5418002792651935\n",
      "Cost after epoch 140: 2.5395092921453215\n",
      "Cost after epoch 150: 2.5367779449998356\n",
      "Cost after epoch 160: 2.53359903052112\n",
      "Cost after epoch 170: 2.5298627786646684\n",
      "Cost after epoch 180: 2.5254852650298827\n",
      "Cost after epoch 190: 2.520385103542658\n",
      "Cost after epoch 200: 2.514480735181132\n",
      "Cost after epoch 210: 2.5078252081366013\n",
      "Cost after epoch 220: 2.5002739365719373\n",
      "Cost after epoch 230: 2.4917707080938416\n",
      "Cost after epoch 240: 2.4822997003998752\n",
      "Cost after epoch 250: 2.471883271217157\n",
      "Cost after epoch 260: 2.460802085423352\n",
      "Cost after epoch 270: 2.449021361274672\n",
      "Cost after epoch 280: 2.436692621291511\n",
      "Cost after epoch 290: 2.4240392249438214\n",
      "Cost after epoch 300: 2.411321743464161\n",
      "Cost after epoch 310: 2.399037959718544\n",
      "Cost after epoch 320: 2.3872314994829527\n",
      "Cost after epoch 330: 2.3760855410660215\n",
      "Cost after epoch 340: 2.3657463627570197\n",
      "Cost after epoch 350: 2.3562846701836375\n",
      "Cost after epoch 360: 2.3478421019208353\n",
      "Cost after epoch 370: 2.3401869126431225\n",
      "Cost after epoch 380: 2.333187836569559\n",
      "Cost after epoch 390: 2.326704089123094\n",
      "Cost after epoch 400: 2.320583485374305\n",
      "Cost after epoch 410: 2.3147805396725367\n",
      "Cost after epoch 420: 2.3090604506084342\n",
      "Cost after epoch 430: 2.303293459649412\n",
      "Cost after epoch 440: 2.2973800155541726\n",
      "Cost after epoch 450: 2.2912409592354126\n",
      "Cost after epoch 460: 2.284937142749056\n",
      "Cost after epoch 470: 2.2783401026131926\n",
      "Cost after epoch 480: 2.2714233462910536\n",
      "Cost after epoch 490: 2.2641912478167256\n",
      "Cost after epoch 500: 2.2566643111429783\n",
      "Cost after epoch 510: 2.2490187616656128\n",
      "Cost after epoch 520: 2.2411790966309946\n",
      "Cost after epoch 530: 2.233177210212223\n",
      "Cost after epoch 540: 2.2250633273339138\n",
      "Cost after epoch 550: 2.2168878700480676\n",
      "Cost after epoch 560: 2.2088467484268386\n",
      "Cost after epoch 570: 2.20085142788015\n",
      "Cost after epoch 580: 2.19292315198268\n",
      "Cost after epoch 590: 2.1850949460341837\n",
      "Cost after epoch 600: 2.1773953491784157\n",
      "Cost after epoch 610: 2.1699835917431116\n",
      "Cost after epoch 620: 2.1627539459841674\n",
      "Cost after epoch 630: 2.155708069822842\n",
      "Cost after epoch 640: 2.1488604779407185\n",
      "Cost after epoch 650: 2.142223472147785\n",
      "Cost after epoch 660: 2.1359211383707137\n",
      "Cost after epoch 670: 2.129851675260064\n",
      "Cost after epoch 680: 2.1240078521054775\n",
      "Cost after epoch 690: 2.118393730974368\n",
      "Cost after epoch 700: 2.113011633414892\n",
      "Cost after epoch 710: 2.1079532272827795\n",
      "Cost after epoch 720: 2.103128111669439\n",
      "Cost after epoch 730: 2.0985237205499905\n",
      "Cost after epoch 740: 2.0941369880154883\n",
      "Cost after epoch 750: 2.0899639485937342\n",
      "Cost after epoch 760: 2.086069746709952\n",
      "Cost after epoch 770: 2.082379459913695\n",
      "Cost after epoch 780: 2.078879562436625\n",
      "Cost after epoch 790: 2.075564413484109\n",
      "Cost after epoch 800: 2.072428183663485\n",
      "Cost after epoch 810: 2.0695170412498003\n",
      "Cost after epoch 820: 2.0667725032578765\n",
      "Cost after epoch 830: 2.064182801621584\n",
      "Cost after epoch 840: 2.0617423249829163\n",
      "Cost after epoch 850: 2.059445514619247\n",
      "Cost after epoch 860: 2.0573247742134004\n",
      "Cost after epoch 870: 2.055336112408675\n",
      "Cost after epoch 880: 2.0534700318966155\n",
      "Cost after epoch 890: 2.0517216000055516\n",
      "Cost after epoch 900: 2.050085974503685\n",
      "Cost after epoch 910: 2.048585177036779\n",
      "Cost after epoch 920: 2.047186961688965\n",
      "Cost after epoch 930: 2.0458838608865038\n",
      "Cost after epoch 940: 2.044671693441127\n",
      "Cost after epoch 950: 2.0435463860283996\n",
      "Cost after epoch 960: 2.0425222109977\n",
      "Cost after epoch 970: 2.041576217363108\n",
      "Cost after epoch 980: 2.0407027068377754\n",
      "Cost after epoch 990: 2.0398982830343835\n",
      "Cost after epoch 1000: 2.0391596669428553\n",
      "Cost after epoch 1010: 2.0384954930716237\n",
      "Cost after epoch 1020: 2.0378900489838148\n",
      "Cost after epoch 1030: 2.0373391371879586\n",
      "Cost after epoch 1040: 2.036840096852474\n",
      "Cost after epoch 1050: 2.03639036856161\n",
      "Cost after epoch 1060: 2.0359944938797896\n",
      "Cost after epoch 1070: 2.035642249944394\n",
      "Cost after epoch 1080: 2.035330610369126\n",
      "Cost after epoch 1090: 2.0350574941332997\n",
      "Cost after epoch 1100: 2.0348208918126436\n",
      "Cost after epoch 1110: 2.034622345484018\n",
      "Cost after epoch 1120: 2.0344556835863643\n",
      "Cost after epoch 1130: 2.0343187284208395\n",
      "Cost after epoch 1140: 2.0342098074594475\n",
      "Cost after epoch 1150: 2.0341272961720005\n",
      "Cost after epoch 1160: 2.0340705777129027\n",
      "Cost after epoch 1170: 2.0340364135029105\n",
      "Cost after epoch 1180: 2.034023234726272\n",
      "Cost after epoch 1190: 2.03402966598615\n",
      "Cost after epoch 1200: 2.0340543694935116\n",
      "Cost after epoch 1210: 2.034095278794512\n",
      "Cost after epoch 1220: 2.034151242870607\n",
      "Cost after epoch 1230: 2.0342211524688296\n",
      "Cost after epoch 1240: 2.0343038815974697\n",
      "Cost after epoch 1250: 2.034398339938808\n",
      "Cost after epoch 1260: 2.0345016122966566\n",
      "Cost after epoch 1270: 2.0346139889344714\n",
      "Cost after epoch 1280: 2.034734725963791\n",
      "Cost after epoch 1290: 2.0348629288133115\n",
      "Cost after epoch 1300: 2.0349977383901137\n",
      "Cost after epoch 1310: 2.0351358674626017\n",
      "Cost after epoch 1320: 2.0352785427380584\n",
      "Cost after epoch 1330: 2.035425312909591\n",
      "Cost after epoch 1340: 2.0355754982559873\n",
      "Cost after epoch 1350: 2.035728452749636\n",
      "Cost after epoch 1360: 2.0358808636140755\n",
      "Cost after epoch 1370: 2.0360345001207905\n",
      "Cost after epoch 1380: 2.0361891378245804\n",
      "Cost after epoch 1390: 2.0363442875007434\n",
      "Cost after epoch 1400: 2.036499490118277\n",
      "Cost after epoch 1410: 2.036651637261842\n",
      "Cost after epoch 1420: 2.036802743463595\n",
      "Cost after epoch 1430: 2.0369527519724033\n",
      "Cost after epoch 1440: 2.037101334723289\n",
      "Cost after epoch 1450: 2.037248189630278\n",
      "Cost after epoch 1460: 2.0373905491954662\n",
      "Cost after epoch 1470: 2.037530461504823\n",
      "Cost after epoch 1480: 2.0376679878053694\n",
      "Cost after epoch 1490: 2.0378029317185637\n",
      "Cost after epoch 1500: 2.0379351186258323\n",
      "Cost after epoch 1510: 2.0380621875165286\n",
      "Cost after epoch 1520: 2.0381860808725576\n",
      "Cost after epoch 1530: 2.038306938496723\n",
      "Cost after epoch 1540: 2.0384246683294798\n",
      "Cost after epoch 1550: 2.038539195979582\n",
      "Cost after epoch 1560: 2.0386485791885756\n",
      "Cost after epoch 1570: 2.0387545734383488\n",
      "Cost after epoch 1580: 2.038857366198396\n",
      "Cost after epoch 1590: 2.038956944308059\n",
      "Cost after epoch 1600: 2.0390533081857742\n",
      "Cost after epoch 1610: 2.0391449074387733\n",
      "Cost after epoch 1620: 2.0392332725547897\n",
      "Cost after epoch 1630: 2.03931861400208\n",
      "Cost after epoch 1640: 2.0394009734321883\n",
      "Cost after epoch 1650: 2.0394804019717943\n",
      "Cost after epoch 1660: 2.039555687954927\n",
      "Cost after epoch 1670: 2.039628129882674\n",
      "Cost after epoch 1680: 2.039697941347977\n",
      "Cost after epoch 1690: 2.0397651963592085\n",
      "Cost after epoch 1700: 2.039829974499716\n",
      "Cost after epoch 1710: 2.0398913360286985\n",
      "Cost after epoch 1720: 2.039950361370293\n",
      "Cost after epoch 1730: 2.040007251937607\n",
      "Cost after epoch 1740: 2.040062094550699\n",
      "Cost after epoch 1750: 2.0401149782575425\n",
      "Cost after epoch 1760: 2.0401651663100573\n",
      "Cost after epoch 1770: 2.0402135476924057\n",
      "Cost after epoch 1780: 2.0402603012373954\n",
      "Cost after epoch 1790: 2.040305511409091\n",
      "Cost after epoch 1800: 2.040349262396773\n",
      "Cost after epoch 1810: 2.0403909583862316\n",
      "Cost after epoch 1820: 2.04043132960081\n",
      "Cost after epoch 1830: 2.040470526872618\n",
      "Cost after epoch 1840: 2.0405086223377524\n",
      "Cost after epoch 1850: 2.040545686307907\n",
      "Cost after epoch 1860: 2.040581214180142\n",
      "Cost after epoch 1870: 2.0406158108101335\n",
      "Cost after epoch 1880: 2.0406495980063286\n",
      "Cost after epoch 1890: 2.040682630641538\n",
      "Cost after epoch 1900: 2.0407149610888884\n",
      "Cost after epoch 1910: 2.0407461404963905\n",
      "Cost after epoch 1920: 2.040776678612993\n",
      "Cost after epoch 1930: 2.0408066704802503\n",
      "Cost after epoch 1940: 2.0408361528881187\n",
      "Cost after epoch 1950: 2.040865160134432\n",
      "Cost after epoch 1960: 2.0408932771359845\n",
      "Cost after epoch 1970: 2.0409209434146023\n",
      "Cost after epoch 1980: 2.0409482315064693\n",
      "Cost after epoch 1990: 2.0409751621751755\n",
      "Cost after epoch 2000: 2.041001754160304\n",
      "Cost after epoch 2010: 2.041027615235042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2020: 2.0410531334272948\n",
      "Cost after epoch 2030: 2.041078363913082\n",
      "Cost after epoch 2040: 2.0411033151529243\n",
      "Cost after epoch 2050: 2.041127994304777\n",
      "Cost after epoch 2060: 2.0411520290299894\n",
      "Cost after epoch 2070: 2.0411757688090604\n",
      "Cost after epoch 2080: 2.0411992566288806\n",
      "Cost after epoch 2090: 2.041222493002015\n",
      "Cost after epoch 2100: 2.0412454779325944\n",
      "Cost after epoch 2110: 2.041267860715978\n",
      "Cost after epoch 2120: 2.041289960461876\n",
      "Cost after epoch 2130: 2.041311812538795\n",
      "Cost after epoch 2140: 2.0413334136525676\n",
      "Cost after epoch 2150: 2.041354760714693\n",
      "Cost after epoch 2160: 2.0413755281970296\n",
      "Cost after epoch 2170: 2.0413960095014487\n",
      "Cost after epoch 2180: 2.0414162359062233\n",
      "Cost after epoch 2190: 2.041436203587472\n",
      "Cost after epoch 2200: 2.0414559094449904\n",
      "Cost after epoch 2210: 2.041475056190731\n",
      "Cost after epoch 2220: 2.0414939138774706\n",
      "Cost after epoch 2230: 2.0415125119230613\n",
      "Cost after epoch 2240: 2.041530847979012\n",
      "Cost after epoch 2250: 2.04154892067848\n",
      "Cost after epoch 2260: 2.041566462170863\n",
      "Cost after epoch 2270: 2.041583720217696\n",
      "Cost after epoch 2280: 2.0416007232724187\n",
      "Cost after epoch 2290: 2.041617471084362\n",
      "Cost after epoch 2300: 2.041633964380275\n",
      "Cost after epoch 2310: 2.041649963671023\n",
      "Cost after epoch 2320: 2.0416656949043004\n",
      "Cost after epoch 2330: 2.041681185321942\n",
      "Cost after epoch 2340: 2.041696436197398\n",
      "Cost after epoch 2350: 2.0417114495684094\n",
      "Cost after epoch 2360: 2.0417260114150015\n",
      "Cost after epoch 2370: 2.041740326176905\n",
      "Cost after epoch 2380: 2.041754418853223\n",
      "Cost after epoch 2390: 2.0417682908755084\n",
      "Cost after epoch 2400: 2.0417819441046867\n",
      "Cost after epoch 2410: 2.041795186306372\n",
      "Cost after epoch 2420: 2.0418082006560145\n",
      "Cost after epoch 2430: 2.0418210085166684\n",
      "Cost after epoch 2440: 2.0418336098077754\n",
      "Cost after epoch 2450: 2.0418460045147886\n",
      "Cost after epoch 2460: 2.0418580189338185\n",
      "Cost after epoch 2470: 2.0418698150244525\n",
      "Cost after epoch 2480: 2.0418814091331674\n",
      "Cost after epoch 2490: 2.0418927981339574\n",
      "Cost after epoch 2500: 2.041903978651957\n",
      "Cost after epoch 2510: 2.0419147937683935\n",
      "Cost after epoch 2520: 2.0419253838016163\n",
      "Cost after epoch 2530: 2.041935758970647\n",
      "Cost after epoch 2540: 2.041945912015654\n",
      "Cost after epoch 2550: 2.041955835210306\n",
      "Cost after epoch 2560: 2.04196538891167\n",
      "Cost after epoch 2570: 2.0419746905881033\n",
      "Cost after epoch 2580: 2.041983743609092\n",
      "Cost after epoch 2590: 2.0419925360854427\n",
      "Cost after epoch 2600: 2.0420010555603505\n",
      "Cost after epoch 2610: 2.042009182683499\n",
      "Cost after epoch 2620: 2.042017009842351\n",
      "Cost after epoch 2630: 2.042024533226296\n",
      "Cost after epoch 2640: 2.042031736420637\n",
      "Cost after epoch 2650: 2.0420386024540376\n",
      "Cost after epoch 2660: 2.0420450376920303\n",
      "Cost after epoch 2670: 2.0420511060868622\n",
      "Cost after epoch 2680: 2.042056796636105\n",
      "Cost after epoch 2690: 2.0420620890103893\n",
      "Cost after epoch 2700: 2.042066962426875\n",
      "Cost after epoch 2710: 2.042071356359681\n",
      "Cost after epoch 2720: 2.042075301908246\n",
      "Cost after epoch 2730: 2.0420787810791996\n",
      "Cost after epoch 2740: 2.0420817706011087\n",
      "Cost after epoch 2750: 2.042084246917074\n",
      "Cost after epoch 2760: 2.042086191335061\n",
      "Cost after epoch 2770: 2.0420875968287744\n",
      "Cost after epoch 2780: 2.0420884387201252\n",
      "Cost after epoch 2790: 2.0420886919704544\n",
      "Cost after epoch 2800: 2.0420883314598695\n",
      "Cost after epoch 2810: 2.042087389665006\n",
      "Cost after epoch 2820: 2.0420858156232544\n",
      "Cost after epoch 2830: 2.042083578294981\n",
      "Cost after epoch 2840: 2.0420806521041204\n",
      "Cost after epoch 2850: 2.0420770116080584\n",
      "Cost after epoch 2860: 2.042072749682575\n",
      "Cost after epoch 2870: 2.0420677654589854\n",
      "Cost after epoch 2880: 2.042062021815552\n",
      "Cost after epoch 2890: 2.042055493803864\n",
      "Cost after epoch 2900: 2.0420481568133644\n",
      "Cost after epoch 2910: 2.0420401726097817\n",
      "Cost after epoch 2920: 2.042031384680142\n",
      "Cost after epoch 2930: 2.042021750024221\n",
      "Cost after epoch 2940: 2.0420112453354826\n",
      "Cost after epoch 2950: 2.04199984782451\n",
      "Cost after epoch 2960: 2.0419877953851313\n",
      "Cost after epoch 2970: 2.041974870715423\n",
      "Cost after epoch 2980: 2.0419610250570166\n",
      "Cost after epoch 2990: 2.041946237556138\n",
      "Cost after epoch 3000: 2.041930488021113\n",
      "Cost after epoch 3010: 2.0419140960896587\n",
      "Cost after epoch 3020: 2.0418967794232668\n",
      "Cost after epoch 3030: 2.041878483553454\n",
      "Cost after epoch 3040: 2.0418591906638826\n",
      "Cost after epoch 3050: 2.041838883706872\n",
      "Cost after epoch 3060: 2.041817967833789\n",
      "Cost after epoch 3070: 2.0417960925458076\n",
      "Cost after epoch 3080: 2.0417731976627094\n",
      "Cost after epoch 3090: 2.0417492687649137\n",
      "Cost after epoch 3100: 2.0417242922694645\n",
      "Cost after epoch 3110: 2.041698760676949\n",
      "Cost after epoch 3120: 2.04167225347155\n",
      "Cost after epoch 3130: 2.0416447047382182\n",
      "Cost after epoch 3140: 2.0416161036061715\n",
      "Cost after epoch 3150: 2.041586440073371\n",
      "Cost after epoch 3160: 2.0415562939193177\n",
      "Cost after epoch 3170: 2.0415251740826093\n",
      "Cost after epoch 3180: 2.0414930088974788\n",
      "Cost after epoch 3190: 2.041459791015825\n",
      "Cost after epoch 3200: 2.041425513959172\n",
      "Cost after epoch 3210: 2.0413908429309653\n",
      "Cost after epoch 3220: 2.0413552171106932\n",
      "Cost after epoch 3230: 2.0413185590987415\n",
      "Cost after epoch 3240: 2.041280864900205\n",
      "Cost after epoch 3250: 2.041242131365064\n",
      "Cost after epoch 3260: 2.04120310567047\n",
      "Cost after epoch 3270: 2.041163159263354\n",
      "Cost after epoch 3280: 2.0411222090800516\n",
      "Cost after epoch 3290: 2.0410802542022326\n",
      "Cost after epoch 3300: 2.0410372945119915\n",
      "Cost after epoch 3310: 2.040994154348512\n",
      "Cost after epoch 3320: 2.0409501405700596\n",
      "Cost after epoch 3330: 2.040905164585309\n",
      "Cost after epoch 3340: 2.0408592282032987\n",
      "Cost after epoch 3350: 2.0408123339750057\n",
      "Cost after epoch 3360: 2.0407653774858026\n",
      "Cost after epoch 3370: 2.040717605110469\n",
      "Cost after epoch 3380: 2.040668922940065\n",
      "Cost after epoch 3390: 2.040619335120062\n",
      "Cost after epoch 3400: 2.040568846470498\n",
      "Cost after epoch 3410: 2.0405184170660147\n",
      "Cost after epoch 3420: 2.040467237688714\n",
      "Cost after epoch 3430: 2.040415209396733\n",
      "Cost after epoch 3440: 2.040362338265327\n",
      "Cost after epoch 3450: 2.0403086309724583\n",
      "Cost after epoch 3460: 2.0402551047391992\n",
      "Cost after epoch 3470: 2.040200900248108\n",
      "Cost after epoch 3480: 2.0401459138796203\n",
      "Cost after epoch 3490: 2.0400901532371787\n",
      "Cost after epoch 3500: 2.040033626454179\n",
      "Cost after epoch 3510: 2.0399774002064524\n",
      "Cost after epoch 3520: 2.0399205709914643\n",
      "Cost after epoch 3530: 2.0398630309296326\n",
      "Cost after epoch 3540: 2.039804788771676\n",
      "Cost after epoch 3550: 2.03974585372738\n",
      "Cost after epoch 3560: 2.039687334099111\n",
      "Cost after epoch 3570: 2.0396282883639203\n",
      "Cost after epoch 3580: 2.0395686048516017\n",
      "Cost after epoch 3590: 2.0395082931108193\n",
      "Cost after epoch 3600: 2.039447363082311\n",
      "Cost after epoch 3610: 2.039386956913749\n",
      "Cost after epoch 3620: 2.0393261013210515\n",
      "Cost after epoch 3630: 2.039264681352128\n",
      "Cost after epoch 3640: 2.039202707042663\n",
      "Cost after epoch 3650: 2.0391401887586804\n",
      "Cost after epoch 3660: 2.0390782949121697\n",
      "Cost after epoch 3670: 2.0390160266751263\n",
      "Cost after epoch 3680: 2.0389532663487278\n",
      "Cost after epoch 3690: 2.038890024186604\n",
      "Cost after epoch 3700: 2.0388263107170483\n",
      "Cost after epoch 3710: 2.0387633133587957\n",
      "Cost after epoch 3720: 2.038700013799185\n",
      "Cost after epoch 3730: 2.038636292141492\n",
      "Cost after epoch 3740: 2.03857215863085\n",
      "Cost after epoch 3750: 2.0385076237378015\n",
      "Cost after epoch 3760: 2.038443887048246\n",
      "Cost after epoch 3770: 2.038379916571601\n",
      "Cost after epoch 3780: 2.038315590763845\n",
      "Cost after epoch 3790: 2.0382509196765803\n",
      "Cost after epoch 3800: 2.0381859135440115\n",
      "Cost after epoch 3810: 2.0381217777660474\n",
      "Cost after epoch 3820: 2.0380574721544664\n",
      "Cost after epoch 3830: 2.0379928740596616\n",
      "Cost after epoch 3840: 2.037927993193456\n",
      "Cost after epoch 3850: 2.0378628394136804\n",
      "Cost after epoch 3860: 2.037798618112689\n",
      "Cost after epoch 3870: 2.0377342860039147\n",
      "Cost after epoch 3880: 2.0376697198558005\n",
      "Cost after epoch 3890: 2.0376049289293854\n",
      "Cost after epoch 3900: 2.0375399226009248\n",
      "Cost after epoch 3910: 2.0374759009903602\n",
      "Cost after epoch 3920: 2.0374118223940294\n",
      "Cost after epoch 3930: 2.0373475634964415\n",
      "Cost after epoch 3940: 2.037283133028028\n",
      "Cost after epoch 3950: 2.037218539808935\n",
      "Cost after epoch 3960: 2.037154973986809\n",
      "Cost after epoch 3970: 2.037091399682432\n",
      "Cost after epoch 3980: 2.037027693962852\n",
      "Cost after epoch 3990: 2.0369638649749944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 4000: 2.0368999209347116\n",
      "Cost after epoch 4010: 2.036837037876991\n",
      "Cost after epoch 4020: 2.036774189540339\n",
      "Cost after epoch 4030: 2.0367112538049135\n",
      "Cost after epoch 4040: 2.0366482382039353\n",
      "Cost after epoch 4050: 2.03658515032289\n",
      "Cost after epoch 4060: 2.0365231484869586\n",
      "Cost after epoch 4070: 2.036461219399717\n",
      "Cost after epoch 4080: 2.036399242146197\n",
      "Cost after epoch 4090: 2.0363372236343937\n",
      "Cost after epoch 4100: 2.036275170811452\n",
      "Cost after epoch 4110: 2.036214221214284\n",
      "Cost after epoch 4120: 2.0361533774240264\n",
      "Cost after epoch 4130: 2.036092520081897\n",
      "Cost after epoch 4140: 2.036031655474123\n",
      "Cost after epoch 4150: 2.0359707899159534\n",
      "Cost after epoch 4160: 2.035911037565789\n",
      "Cost after epoch 4170: 2.0358514193778086\n",
      "Cost after epoch 4180: 2.0357918178597565\n",
      "Cost after epoch 4190: 2.0357322386912\n",
      "Cost after epoch 4200: 2.0356726875730726\n",
      "Cost after epoch 4210: 2.035614253148805\n",
      "Cost after epoch 4220: 2.0355559768469362\n",
      "Cost after epoch 4230: 2.035497743311522\n",
      "Cost after epoch 4240: 2.035439557639299\n",
      "Cost after epoch 4250: 2.035381424942739\n",
      "Cost after epoch 4260: 2.0353244066308736\n",
      "Cost after epoch 4270: 2.0352675663400897\n",
      "Cost after epoch 4280: 2.0352107910811217\n",
      "Cost after epoch 4290: 2.035154085397936\n",
      "Cost after epoch 4300: 2.035097453846226\n",
      "Cost after epoch 4310: 2.0350419292604887\n",
      "Cost after epoch 4320: 2.034986598879098\n",
      "Cost after epoch 4330: 2.034931352272159\n",
      "Cost after epoch 4340: 2.034876193465065\n",
      "Cost after epoch 4350: 2.0348211264921905\n",
      "Cost after epoch 4360: 2.0347671546150354\n",
      "Cost after epoch 4370: 2.0347133897583025\n",
      "Cost after epoch 4380: 2.034659724208667\n",
      "Cost after epoch 4390: 2.034606161509543\n",
      "Cost after epoch 4400: 2.0345527052115697\n",
      "Cost after epoch 4410: 2.034500328309741\n",
      "Cost after epoch 4420: 2.034448168219396\n",
      "Cost after epoch 4430: 2.0343961200679397\n",
      "Cost after epoch 4440: 2.034344186954443\n",
      "Cost after epoch 4450: 2.034292371984171\n",
      "Cost after epoch 4460: 2.0342416174618907\n",
      "Cost after epoch 4470: 2.0341910868470974\n",
      "Cost after epoch 4480: 2.0341406782017204\n",
      "Cost after epoch 4490: 2.0340903942180697\n",
      "Cost after epoch 4500: 2.0340402375941533\n",
      "Cost after epoch 4510: 2.0339911197573772\n",
      "Cost after epoch 4520: 2.033942230542352\n",
      "Cost after epoch 4530: 2.033893471011704\n",
      "Cost after epoch 4540: 2.033844843487711\n",
      "Cost after epoch 4550: 2.0337963502982195\n",
      "Cost after epoch 4560: 2.033748872011766\n",
      "Cost after epoch 4570: 2.033701624973296\n",
      "Cost after epoch 4580: 2.033654513287463\n",
      "Cost after epoch 4590: 2.033607538941782\n",
      "Cost after epoch 4600: 2.033560703929466\n",
      "Cost after epoch 4610: 2.033514858155739\n",
      "Cost after epoch 4620: 2.0334692444405333\n",
      "Cost after epoch 4630: 2.0334237699497146\n",
      "Cost after epoch 4640: 2.0333784363694622\n",
      "Cost after epoch 4650: 2.033333245391924\n",
      "Cost after epoch 4660: 2.0332890166055604\n",
      "Cost after epoch 4670: 2.0332450191226865\n",
      "Cost after epoch 4680: 2.0332011631700677\n",
      "Cost after epoch 4690: 2.0331574501638596\n",
      "Cost after epoch 4700: 2.033113881526546\n",
      "Cost after epoch 4710: 2.0330712470036887\n",
      "Cost after epoch 4720: 2.033028841691592\n",
      "Cost after epoch 4730: 2.032986578860576\n",
      "Cost after epoch 4740: 2.0329444596858552\n",
      "Cost after epoch 4750: 2.032902485349354\n",
      "Cost after epoch 4760: 2.032861416333657\n",
      "Cost after epoch 4770: 2.0328205733043747\n",
      "Cost after epoch 4780: 2.032779872543367\n",
      "Cost after epoch 4790: 2.0327393150117046\n",
      "Cost after epoch 4800: 2.0326989016775467\n",
      "Cost after epoch 4810: 2.0326593644274427\n",
      "Cost after epoch 4820: 2.0326200489929973\n",
      "Cost after epoch 4830: 2.032580874623077\n",
      "Cost after epoch 4840: 2.032541842089162\n",
      "Cost after epoch 4850: 2.0325029521701694\n",
      "Cost after epoch 4860: 2.032464908893666\n",
      "Cost after epoch 4870: 2.032427082481152\n",
      "Cost after epoch 4880: 2.0323893950934453\n",
      "Cost after epoch 4890: 2.0323518473348177\n",
      "Cost after epoch 4900: 2.0323144398172843\n",
      "Cost after epoch 4910: 2.0322778495016456\n",
      "Cost after epoch 4920: 2.032241470464542\n",
      "Cost after epoch 4930: 2.0322052277149805\n",
      "Cost after epoch 4940: 2.032169121710347\n",
      "Cost after epoch 4950: 2.03213315291603\n",
      "Cost after epoch 4960: 2.032097972060293\n",
      "Cost after epoch 4970: 2.0320629963940466\n",
      "Cost after epoch 4980: 2.032028153703644\n",
      "Cost after epoch 4990: 2.0319934443179823\n",
      "training time: 0:00:00.930730\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAfv0lEQVR4nO3deZRc5Xnn8e9T1bu61Yu6hUQvtBBiE1rdgNkchzi2wbGxMbbx2Bhvh7Enk8AJWbxkPE4ynhySGUywM8PBxmATYjs2mHE88WCMISDLCLWEhBAyaAW0t9Raulvqpaqe+ePelgrR3eqt6tby+5xTp+5W1c/batWv7n3vfa+5OyIiUrxiURcgIiLRUhCIiBQ5BYGISJFTEIiIFDkFgYhIkSuJuoCJamxs9Pb29qjLEBHJK2vWrDng7k0jrcu7IGhvb6ezszPqMkRE8oqZvTraOh0aEhEpcgoCEZEipyAQESlyCgIRkSKnIBARKXIKAhGRIqcgEBEpcnl3HcFkbevq5dHnd7GwuZZFzbXMra3AzKIuS0QkckUTBBt3H+WbT24hFd5+oammnGsumsPHLj2L8+bURFuciEiELN9uTNPR0eGTvbL4+GCSTXuPsnHXEZ7d1s0vN+1jKJniY5eexZffcwEVpfFprlZEJDeY2Rp37xhpXdHsEQBUlsVZ3lbP8rZ6brqsncPHBrnrl5t5YOUONuw6wvc+cwkzK0qjLlNEJKuKurO4rqqMr75vIfd8fDkv7jrC5x5cQzKVX3tIIiJTVdRBMOzdF83lv1+/iJVbD3LPv2+NuhwRkaxSEIQ+9JYWrl00h7uf2MyeI8ejLkdEJGsUBCEz44vXXIA7/MMvN0ddjohI1igI0rQ2VHHjJa08snYXXT0DUZcjIpIVCoJT3Hx5O4PJFD9c/VrUpYiIZIWC4BTzm6q5fP4sfrRmJ/l2jYWIyGQoCEbwviVn8urBY2zcfTTqUkREMk5BMIJ3LpxDPGb824Y9UZciIpJxCoIRNMwo4+L2ep56uSvqUkREMk5BMIqrFjTx0p6jHOzV2UMiUtgUBKO44pxGAFZuPRhxJSIimaUgGMWi5lpmVpSwcuuBqEsREckoBcEo4jFj+Vn1rH31cNSliIhkVMaCwMxazexJM9tkZhvN7NYRtnm7mR0xs3Xh4yuZqmcylrXW88r+Hnr6h6IuRUQkYzJ5P4IEcLu7rzWzGmCNmT3u7i+dst0z7v4HGaxj0pa21eEOG3Ye4fKwz0BEpNBkbI/A3fe4+9pwugfYBDRn6udlwtKWOgCef12Hh0SkcGWlj8DM2oFlwKoRVl9mZuvN7OdmtnCU199iZp1m1tnVlb1z+2urSjm7cQbrFAQiUsAyHgRmVg08DNzm7qeO2bAWOMvdlwDfAB4d6T3c/V5373D3jqampswWfIoL5s7k5b09Wf2ZIiLZlNEgMLNSghB4yN0fOXW9ux91995w+t+AUjPLqYPx582p4bXuY/QNJKIuRUQkIzJ51pAB9wGb3P3OUbaZE26HmV0S1pNTV3CdN6cGgFf2aa9ARApTJs8augK4CdhgZuvCZV8C2gDc/R7gBuDzZpYAjgM3eo6N/Xx+GAQv7+1hWVt9xNWIiEy/jAWBu68A7DTbfBP4ZqZqmA6t9VVUlcX5rfoJRKRA6cri04jFjHPPqFGHsYgULAXBOJwzu5ptB3qjLkNEJCMUBOMwr3EG+44O6MwhESlICoJxOLtxBgDbD/RFXImIyPRTEIzDvCYFgYgULgXBOLTPUhCISOFSEIxDRWmc5rpKBYGIFCQFwTjNa5zBNgWBiBQgBcE4zWucwfauXnLswmcRkSlTEIzTvMYZHO1P0N03GHUpIiLTSkEwTmfNqgLgte5jEVciIjK9FATj1NqgIBCRwqQgGKeW+koAdh46HnElIiLTS0EwTlVlJTRWl/G69ghEpMAoCCagpb6K1w8pCESksCgIJqCtoUp9BCJScBQEE9DaUMnuw/0kkqmoSxERmTYKgglora8imXL2HOmPuhQRkWmjIJiA4VNI1U8gIoVEQTABbcNBoH4CESkgCoIJmFtbQTxmvN6tawlEpHAoCCagJB5jbm2FDg2JSEFREExQa32VDg2JSEFREExQcC2BDg2JSOFQEExQ26wqDvQOcHwwGXUpIiLTQkEwQTqFVEQKjYJggoZPIX3toIJARAqDgmCC2nRfAhEpMAqCCaqvKqW6vERBICIFI2NBYGatZvakmW0ys41mdusY215sZkkzuyFT9UwXM6OlvlKnkIpIwSjJ4HsngNvdfa2Z1QBrzOxxd38pfSMziwN3AI9lsJZp1dZQxfYDfVGXISIyLTK2R+Due9x9bTjdA2wCmkfY9I+Ah4H9maplug3fl8Ddoy5FRGTKstJHYGbtwDJg1SnLm4EPAPec5vW3mFmnmXV2dXVlqsxxa5tVxUAiRVfPQNSliIhMWcaDwMyqCb7x3+buR09ZfRfwF+4+5tVZ7n6vu3e4e0dTU1OmSh23Vp05JCIFJJN9BJhZKUEIPOTuj4ywSQfwAzMDaASuNbOEuz+aybqmKv0U0o72hoirERGZmowFgQWf7vcBm9z9zpG2cfd5ads/APws10MAoLmuEjPtEYhIYcjkHsEVwE3ABjNbFy77EtAG4O5j9gvksorSOHNmVui+BCJSEDIWBO6+ArAJbP/JTNWSCa0NGo5aRAqDriyepOFTSEVE8p2CYJLaGqrYe7Rfw1GLSN5TEEzS/KZqALYd6I24EhGRqVEQTNL82TMA2NqloSZEJL8pCCapfdYMYgZb92uPQETym4JgkipK47Q2VLGlS0EgIvlNQTAF85uqtUcgInlPQTAF58yuZvuBPpIpjUIqIvlLQTAF85tmMJBIseuQrjAWkfylIJiC4VNIt3T1RFyJiMjkKQim4Nw5NQBs2qMgEJH8pSCYgpkVpbQ1VPHS7lNvsyAikj8UBFN0UfNMXtx9JOoyREQmTUEwRQvPrOXVg8c42j8UdSkiIpOiIJiihWfOBNDhIRHJWwqCKVp4Zi0AL+7S4SERyU8Kgilqqimnua6S5187HHUpIiKToiCYBhe317N6RzfuusJYRPKPgmAadLQ3sL9nQPcwFpG8pCCYBh3t9QB0vtodcSUiIhOnIJgG586uoaaihNU7DkVdiojIhCkIpkEsZnScVc9z2w9GXYqIyIQpCKbJ5fMb2drVx54j6icQkfyiIJgmVy5oBGDF5gMRVyIiMjEKgmly/pwaGqvLeUZBICJ5RkEwTcyMqxY0smLLAVK6Y5mI5BEFwTS6akEj3X2DvLRH4w6JSP4YVxCY2YfGs6zYXXlO0E+gw0Mikk/Gu0fwxXEuK2qzZ1Zw3hk1rNjSFXUpIiLjVjLWSjO7BrgWaDazu9NWzQQSp3ltK/A9YA6QAu519384ZZvrgL8J1yeA29x9xUQbkUuuXNDIg8++Sv9QkorSeNTliIic1un2CHYDnUA/sCbt8VPgXad5bQK43d0vAN4K/KGZXXjKNk8AS9x9KfBp4NsTKz/3XLmgkcFEitU7NNyEiOSHMfcI3H09sN7M/tndhwDMrB5odfcxx1Nw9z3AnnC6x8w2Ac3AS2nb9Ka9ZAaQ96fbXDqvgdK4sWLLAa5a0BR1OSIipzXePoLHzWymmTUA64H7zezO8f4QM2sHlgGrRlj3ATP7LfB/CfYKRnr9LWbWaWadXV25ffy9qqyE5W31/HqLOoxFJD+MNwhq3f0ocD1wv7u/BXjHeF5oZtXAwwTH/990XqW7/8TdzwfeT9Bf8Cbufq+7d7h7R1NT7n/LvvKcRjbuPkp332DUpYiInNZ4g6DEzOYCHwZ+Nt43N7NSghB4yN0fGWtbd38amG9mjeN9/1x15YJG3GHlVu0ViEjuG28Q/DXwGLDV3Veb2dnA5rFeYGYG3AdscvcRDyOZ2TnhdpjZcqAMyPshPBc111JTUaJxh0QkL4zZWTzM3X8E/ChtfhvwwdO87ArgJmCDma0Ll30JaAvf457wPT5hZkPAceAjXgD3eyyJx7h8/ixdWCYieWFcQWBmLcA3CD7cHVgB3OruO0d7TXg9gI31vu5+B3DHuKvNI5edPYvHNu5j1+HjNNdVRl2OiMioxnto6H6CawfOJDgF9F/DZTKKi+c1ALB6u64nEJHcNt4gaHL3+909ET4eAHL/9J0InT9nJjXlJbqwTERy3niD4ICZfdzM4uHj4xRAp24mxWPG8rPqFQQikvPGGwSfJjh1dC/B1cI3AJ/KVFGF4uL2el7Z18vhY7qeQERy13iD4G+Am929yd1nEwTDVzNWVYG4uD3oJ+jcMeZoHCIikRpvECxOH1vI3bsJhoyQMSxpraM0bqx+VYeHRCR3jTcIYuFgcwCEYw6N69TTYlZRGuei5lrWaI9ARHLYeD/M/yew0sx+THAdwYeBr2WsqgKytLWO7z/3GolkipK47gwqIrlnXJ9M7v49gquA9wFdwPXu/mAmCysUS1vr6B9KsXl/7+k3FhGJwLgP77j7S6TdS0DGZ0lLHQDrXz/MBXNnRlyNiMib6VhFhp01q4raylLW7zwcdSkiIiNSEGSYmbG4pZZ1rx+JuhQRkREpCLJgaWsdr+zr4dhgIupSRETeREGQBUta6kimnI2733SDNhGRyCkIsmBxay0QdBiLiOQaBUEWzK6poLmukvU71U8gIrlHQZAli1tqtUcgIjlJQZAli1vqeK37GIf6NBKpiOQWBUGWLGkJ+gk27NLhIRHJLQqCLFnYHATBC7qwTERyjIIgS2orSzm7cQYvqMNYRHKMgiCLFrXUKghEJOcoCLJocUsde4/2s/9of9SliIicoCDIosUtw/0E2isQkdyhIMiihWfOJGbqMBaR3KIgyKKqshLOPaOGF3QKqYjkEAVBli1qDjqM3T3qUkREAAVB1i1uraO7b5Bdh49HXYqICKAgyLrFzeowFpHckrEgMLNWM3vSzDaZ2UYzu3WEbT5mZi+Ej5VmtiRT9eSK8+fWUBo33bpSRHLGuG9ePwkJ4HZ3X2tmNcAaM3vc3V9K22Y78DvufsjMrgHuBS7NYE2RKy+Jc8HcmWzQHoGI5IiM7RG4+x53XxtO9wCbgOZTtlnp7ofC2WeBlkzVk0sWNdeyYecRUil1GItI9LLSR2Bm7cAyYNUYm30G+Pkor7/FzDrNrLOrq2v6C8yyJS119Awk2HGwL+pSREQyHwRmVg08DNzm7iPetNfMfpcgCP5ipPXufq+7d7h7R1NTU+aKzZJFusJYRHJIRoPAzEoJQuAhd39klG0WA98GrnP3g5msJ1csmF1NRWlMHcYikhMyedaQAfcBm9z9zlG2aQMeAW5y91cyVUuuKYnHWHhmrTqMRSQnZPKsoSuAm4ANZrYuXPYloA3A3e8BvgLMAv5XkBsk3L0jgzXljMUttXz/uddIJFOUxHU5h4hEJ2NB4O4rADvNNp8FPpupGnLZkpY67v/1DrZ09XL+nJlRlyMiRUxfRSNyosP4dR0eEpFoKQgiMm/WDGrKS9RhLCKRUxBEJBYzLmquZYOGpBaRiCkIIrS4tZZNe44ykEhGXYqIFDEFQYSWtNQxlHRe3tsTdSkiUsQUBBFaFA5JvV7XE4hIhBQEEWqpr6RhRhkvvK4OYxGJjoIgQmYWjESqDmMRiZCCIGJLWmp5ZV8PxwYTUZciIkVKQRCxxS11pBw27h5xYFYRkYxTEERsWVsdAJ07Dp1mSxGRzFAQRGxWdTnnzK7mue1FMQK3iOQgBUEOuLi9gc4dh0jq1pUiEgEFQQ64dF4DPQMJNu1RP4GIZJ+CIAdcMq8BgNU7uiOuRESKkYIgB5xZV0lzXSXPbVcQiEj2KQhyxKXzGnhuezfu6icQkexSEOSIS+Y1cLBvkG0H+qIuRUSKjIIgRwz3E/xmq04jFZHsUhDkiHmNM2iuq+SZzV1RlyIiRUZBkCPMjLed28jKLQdJJFNRlyMiRURBkEOuWtBEz0CCdRqWWkSySEGQQ66Y30jM4OnNB6IuRUSKiIIgh9RWlbKktY6nX1E/gYhkj4Igx7xtQRMv7DzM4WODUZciIkVCQZBj3nZuEynX4SERyR4FQY5Z2lpHY3UZv9i4N+pSRKRIKAhyTDxm/P6FZ/DUy10MJJJRlyMiRUBBkIPeuXAOvQMJVm7RVcYiknkZCwIzazWzJ81sk5ltNLNbR9jmfDP7jZkNmNmfZqqWfHP5/FlUl5fwmA4PiUgWZHKPIAHc7u4XAG8F/tDMLjxlm27gj4H/kcE68k55SZy3n9fE4y/t01XGIpJxGQsCd9/j7mvD6R5gE9B8yjb73X01MJSpOvLVe5ecycG+QZ7ZorOHRCSzstJHYGbtwDJg1SRff4uZdZpZZ1dXcVxs9bvnzaauqpSfrN0VdSkiUuAyHgRmVg08DNzm7pO6Ka+73+vuHe7e0dTUNL0F5qiykhjvWTSXX7y0l96BRNTliEgBy2gQmFkpQQg85O6PZPJnFaLrlzfTP5Ti5xv2RF2KiBSwTJ41ZMB9wCZ3vzNTP6eQLW+r56xZVfxozc6oSxGRApbJPYIrgJuAq81sXfi41sw+Z2afAzCzOWa2E/gT4C/NbKeZzcxgTXnFzPjoJW08t72bl/f2RF2OiBSokky9sbuvAOw02+wFWjJVQyH4SEcrX3/8Fb73mx187QOLoi5HRAqQrizOcfUzyrhu6Zk8snYXR47rLFsRmX4KgjzwycvncXwoyYO/2RF1KSJSgBQEeeDCM2dy9fmzuW/Fdvp0KqmITDMFQZ74o6vP4dCxIf7p2VejLkVECoyCIE8sa6vnqgWN3Pv0No72q69ARKaPgiCP/Pm7zqf72CB3/3Jz1KWISAFREOSRRS21fPgtrTywcgdb9vdGXY6IFAgFQZ75s3efR2VZnC//ZAOplEddjogUAAVBnmmsLue/vOdCVm3v5v6VO6IuR0QKgIIgD32oo4V3XDCbO/7fb9m0Z1IDuoqInKAgyENmxt9ev5j6qlI++91ODvQORF2SiOQxBUGeaqop51uf6OBg3wD/8cE1HBvUhWYiMjkKgjy2uKWOr394Kc+/dohP3b9aYSAik6IgyHPXLJrL1z+ylNU7uvnot1ax72h/1CWJSJ5REBSA65Y2c8/H38KWfT289xsr+LVueC8iE2Du+XUuekdHh3d2dkZdRk56eW8Pn/+nNWw70MeHO1q4/Z3nccbMiqjLmhJ3J5FyBhIpBsNHIpUikQyWJ1NOIpUimXKGkm+cP7lNisQI88nwkfLg56TcSaYg5R7OQzJ1cjrlTtIdd0ilTi47+RhefnKdh++ROs3/s+CGfqOsG+N1JTEjnvYI5mOUxI2Y2Yn1JTEjHg+eh5eXlsQoi8coK4lRXhKnvCRGecnJ+bK0+TdMx2Nj1iu5yczWuHvHiOsUBIWlfyjJXb/czLef2UY8Ftzh7KOXtHHenJqs1eDu9A0mOdg7wIHeQQ72DnD4+BC9/Ql6B9Ie/Sef+wYTJz7sBxJJBhIpBoaC6aivmzODmBkxCz6w4+F0zAwziIcfrpa2PB6zN7wuZjb6J/oY7Rur6R4GUzIMuFQYmslTQjITv7/hYKgsjVNZFqeyNE5F6fBzjMqyk/PD21SkTQ9v9+Zlb5yOxxQ400VBUIRe7z7G3U9s5tF1uxhKOhfMncnbzm3ksrNnceHcmTTVlE/oW10imaL72CAHegY52DfAgd4BDvYOnvigP9g3ePKDv2+A/qHUqO9VURqjuryUmooSqsuDR1X4QVGe9u2zfHg+HqO8NPiWWhoPvu0Of9MtjcfSvgmffr4kdvL1sXB5+gd4zIxYLG3axv62ng/ch0Phjc9DyeHgDQJ3MHFyfjCRYjB5cvlAIn3bYLp/KMlAIkn/UIrjg0mODwWP/qHkifn06ckEUlkYNumhUV4apzKcrygZDplw/sSytPnSOBUlJ8NpeH15STxtWYySeGEfKVcQFLHuvkF+8vwufrFxL2tfO8RQMvj3rq0sZW5tBbOqy6itLCUeixEPP++ODSY5NpikbzD4tn6wb5BDxwYZ6U+lNG7MmlHOrOoyGqvTnmeUMWt4fkY5dVXBB/+M8hJKC/w/nLyZuzOYTNE/mDoRGKeGRX8ifD6xPnVi/bHBBP1DQfj0J1L0p28fhtHwsuG/8YkqjRsVJXEqhoNlOChK4ie+iKQfHitLO2x28hDbCOvjwZeaMbcZ3q4kc4fdFAQCQN9AgvU7D/PK3h427++lqyf4Jn/k+BCp1Mnj31VlcarK4swoL2FGWQmzqoMP9abwOf0Df2ZFSd5/Y5bCkkimgrAIA+bEXssbQicMlROPk+uH5/vT93CGhvuokgwmT/ZXDe85TTZ8RlIaD/Zkg8fJ6ZK48R8uaeOzV509qfcdKwgydvN6yT0zyku4fH4jl89vjLoUkYwpiceojseoLs/ex1sq5eGhtJPh8MawSL7hhIc3rE+++ZBcIgyXwWSKoURwcsNgMkVjdXlG6lcQiIhMUSxmVMSC/oZ8pIO1IiJFTkEgIlLkFAQiIkVOQSAiUuQUBCIiRU5BICJS5BQEIiJFTkEgIlLk8m6ICTPrAl6d5MsbgWIbrF9tLg5qc3GYSpvPcvemkVbkXRBMhZl1jjbWRqFSm4uD2lwcMtVmHRoSESlyCgIRkSJXbEFwb9QFREBtLg5qc3HISJuLqo9ARETerNj2CERE5BQKAhGRIlc0QWBm7zazl81si5l9Iep6psLMvmNm+83sxbRlDWb2uJltDp/rw+VmZneH7X7BzJanvebmcPvNZnZzFG0ZDzNrNbMnzWyTmW00s1vD5YXc5goze87M1odt/qtw+TwzWxXW/0MzKwuXl4fzW8L17Wnv9cVw+ctm9q5oWjR+ZhY3s+fN7GfhfEG32cx2mNkGM1tnZp3hsuz+bbt7wT+AOLAVOBsoA9YDF0Zd1xTa8zZgOfBi2rK/A74QTn8BuCOcvhb4OWDAW4FV4fIGYFv4XB9O10fdtlHaOxdYHk7XAK8AFxZ4mw2oDqdLgVVhW/4FuDFcfg/w+XD6PwH3hNM3Aj8Mpy8M/97LgXnh/4N41O07Tdv/BPhn4GfhfEG3GdgBNJ6yLKt/28WyR3AJsMXdt7n7IPAD4LqIa5o0d38a6D5l8XXAd8Pp7wLvT1v+PQ88C9SZ2VzgXcDj7t7t7oeAx4F3Z776iXP3Pe6+NpzuATYBzRR2m93de8PZ0vDhwNXAj8Plp7Z5+HfxY+D3zMzC5T9w9wF33w5sIfj/kJPMrAV4D/DtcN4o8DaPIqt/28USBM3A62nzO8NlheQMd98DwQcnMDtcPlrb8/J3Eu7+LyP4hlzQbQ4PkawD9hP8x94KHHb3RLhJev0n2hauPwLMIs/aDNwF/DmQCudnUfhtduAXZrbGzG4Jl2X1b7tYbl5vIywrlvNmR2t73v1OzKwaeBi4zd2PBl/+Rt50hGV512Z3TwJLzawO+AlwwUibhc9532Yz+wNgv7uvMbO3Dy8eYdOCaXPoCnffbWazgcfN7LdjbJuRNhfLHsFOoDVtvgXYHVEtmbIv3EUkfN4fLh+t7Xn1OzGzUoIQeMjdHwkXF3Sbh7n7YeApgmPCdWY2/AUuvf4TbQvX1xIcPsynNl8BvM/MdhAcvr2aYA+hkNuMu+8On/cTBP4lZPlvu1iCYDWwIDz7oIygY+mnEdc03X4KDJ8pcDPwf9KWfyI82+CtwJFwV/Mx4J1mVh+ekfDOcFnOCY/73gdscvc701YVcpubwj0BzKwSeAdB38iTwA3hZqe2efh3cQPwKw96EX8K3BieYTMPWAA8l51WTIy7f9HdW9y9neD/6K/c/WMUcJvNbIaZ1QxPE/xNvki2/7aj7jHP1oOgt/0VguOsX466nim25fvAHmCI4JvAZwiOjT4BbA6fG8JtDfjHsN0bgI609/k0QUfaFuBTUbdrjPZeSbCb+wKwLnxcW+BtXgw8H7b5ReAr4fKzCT7UtgA/AsrD5RXh/JZw/dlp7/Xl8HfxMnBN1G0bZ/vfzsmzhgq2zWHb1oePjcOfTdn+29YQEyIiRa5YDg2JiMgoFAQiIkVOQSAiUuQUBCIiRU5BICJS5BQEUvDM7G/N7O1m9n6b4Miz4fn8q8LRMK/KVI2j/Oze028lMnUKAikGlxKMTfQ7wDMTfO3vAb9192XuPtHXiuQFBYEULDP7ezN7AbgY+A3wWeB/m9lXRtj2LDN7Ihzj/QkzazOzpQTDAV8bjhVfecpr3mJm/x4OFvZY2pAAT5nZXWa20sxeNLNLwuUNZvZo+DOeNbPF4fJqM7s/HJP+BTP7YNrP+JoF9yR41szOCJd9KHzf9Wb2dGZ+e1JUor6yTg89MvkgGLflGwTDOP96jO3+Fbg5nP408Gg4/UngmyNsXwqsBJrC+Y8A3wmnnwK+FU6/jfC+EWEd/zWcvhpYF07fAdyV9t714bMD7w2n/w74y3B6A9AcTtdF/TvWI/8fxTL6qBSvZQRDUpwPvDTGdpcB14fTDxJ88I7lPOAigtEiIbj50Z609d+H4N4RZjYzHDfoSuCD4fJfmdksM6slGEfoxuEXejCePMAg8LNweg3w++H0r4EHzOxfgOEB+EQmTUEgBSk8rPMAwSiMB4CqYLGtAy5z9+OneYvTjb1iwEZ3v2ycrx9rqGAb5ecNufvw8iTh/1d3/5yZXUpwA5d1ZrbU3Q+epl6RUamPQAqSu69z96WcvK3lr4B3ufvSUUJgJSe/lX8MWHGaH/Ey0GRml0EwTLaZLUxb/5Fw+ZUEI0QeAZ4O35twvP0D7n4U+AXwn4dfGI4eOSozm+/uq9z9KwQh1zrW9iKnoz0CKVhm1gQccveUmZ3v7mMdGvpj4Dtm9mdAF/Cpsd7b3QfN7Abg7vDwTgnB2Pkbw00OmdlKYCZBnwPAV4H7ww7sY5wcZvi/Af9oZi8SfPP/K8Y+5PP3ZraAYE/iCYKRK0UmTaOPikwzM3sK+FN374y6FpHx0KEhEZEipz0CEZEipz0CEZEipyAQESlyCgIRkSKnIBARKXIKAhGRIvf/AVvSyJpO3LYuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock's neighbor words: ['investing', 'a', 'is', 'beating']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "beating's neighbor words: ['market', 'stock', 'investing', 'costs']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', 'of']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20a8a841f90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "[518.3657183647156, 515.8615536689758, 513.3740382194519, 510.9024748802185, 508.4454679489136, 506.00162506103516, 503.5684814453125, 501.14673233032227, 498.7354040145874, 496.3341040611267]\n"
     ]
    }
   ],
   "source": [
    "#N-Gram Language Modeling\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([17,  8, 29, 45])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing Word Embeddings: Continuous Bag-of-Words\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example of word Embedding using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training dataset\n",
    "sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   # import the corpus and convert into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of corpus:  <class 'list'>\n",
      "Length of corpus:  3106\n"
     ]
    }
   ],
   "source": [
    "print('Type of corpus: ', type(sentences))\n",
    "print('Length of corpus: ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Fran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare']\n",
      "['actus', 'primus']\n",
      "['fran']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('horatio', 0.9972403645515442),\n",
       " ('laertes', 0.9968450665473938),\n",
       " ('looke', 0.9967054128646851),\n",
       " ('how', 0.9966220259666443),\n",
       " ('meane', 0.9963749647140503),\n",
       " ('does', 0.9961333274841309),\n",
       " ('mother', 0.9960773587226868),\n",
       " ('much', 0.996006965637207),\n",
       " ('newes', 0.9959443807601929),\n",
       " ('heere', 0.9959315061569214)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save and load model\n",
    "model.save('word2vec_model')\n",
    "model = Word2Vec.load('word2vec_model')\n",
    "#Similarity calculation\n",
    "model.most_similar('hamlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "v1 = model['king']\n",
    "v2 = model['queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957135915756226"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0529 13:45:30.007943 22736 __init__.py:329] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for sent in corpus:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36, 5, 27, 46, 7], [36, 19, 33, 34, 13, 31, 12], [39, 30, 8, 12, 5, 4], [37, 23], [45, 36, 21, 41, 13, 31, 12], [47, 4, 34, 13], [19, 34, 19, 4, 42, 7], [12, 5, 4, 18, 7], [16, 19], [17, 5, 41], [4, 36], [12, 33, 36, 27], [13, 47, 9, 31, 36, 7], [36, 7, 33, 16], [13, 47, 9, 23], [36, 19, 5, 4]]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences = [one_hot(sent, 50) for sent in corpus]\n",
    "print(embedded_sentences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  5 27 46  7  0  0]\n",
      " [36 19 33 34 13 31 12]\n",
      " [39 30  8 12  5  4  0]\n",
      " [37 23  0  0  0  0  0]\n",
      " [45 36 21 41 13 31 12]\n",
      " [47  4 34 13  0  0  0]\n",
      " [19 34 19  4 42  7  0]\n",
      " [12  5  4 18  7  0  0]\n",
      " [16 19  0  0  0  0  0]\n",
      " [17  5 41  0  0  0  0]\n",
      " [ 4 36  0  0  0  0  0]\n",
      " [12 33 36 27  0  0  0]\n",
      " [13 47  9 31 36  7  0]\n",
      " [36  7 33 16  0  0  0]\n",
      " [13 47  9 23  0  0  0]\n",
      " [36 19  5  4  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:07:02.279598 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0529 14:07:02.333444 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(50, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:07:38.852829 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0529 14:07:38.875306 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0529 14:07:38.879295 22736 deprecation.py:323] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 20)             1000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 141       \n",
      "=================================================================\n",
      "Total params: 1,141\n",
      "Trainable params: 1,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:07:59.795648 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0529 14:07:59.872444 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0529 14:07:59.907349 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0529 14:07:59.909344 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0529 14:07:59.910341 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:08:04.958337 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0529 14:08:04.958337 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0529 14:08:05.681451 22736 module_wrapper.py:139] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 7s 419ms/step - loss: 0.6982 - acc: 0.3750\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6949 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6916 - acc: 0.5625\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6884 - acc: 0.6250\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6851 - acc: 0.8750\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6819 - acc: 0.8750\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6786 - acc: 0.8750\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6754 - acc: 0.8750\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6722 - acc: 0.8750\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.6689 - acc: 0.8750\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6657 - acc: 0.8750\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6625 - acc: 0.8750\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6592 - acc: 0.8750\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6560 - acc: 0.8750\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6527 - acc: 0.8750\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6494 - acc: 0.8750\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.6462 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6429 - acc: 0.8750\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6396 - acc: 0.8750\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6362 - acc: 0.8750\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6329 - acc: 0.8750\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6295 - acc: 0.8750\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6262 - acc: 0.8750\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6228 - acc: 0.8750\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6193 - acc: 0.8750\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.6159 - acc: 0.8750\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6124 - acc: 0.8750\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6089 - acc: 0.8750\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6054 - acc: 0.8750\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.6018 - acc: 0.8750\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.5983 - acc: 0.8750\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5946 - acc: 0.8750\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5910 - acc: 0.8750\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5873 - acc: 0.8750\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5836 - acc: 0.8750\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 233us/step - loss: 0.5799 - acc: 0.8750\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 222us/step - loss: 0.5762 - acc: 0.8750\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5724 - acc: 0.8750\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 221us/step - loss: 0.5685 - acc: 0.8750\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 253us/step - loss: 0.5647 - acc: 0.8750\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 279us/step - loss: 0.5608 - acc: 0.8750\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 215us/step - loss: 0.5569 - acc: 0.8750\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 195us/step - loss: 0.5530 - acc: 0.8750\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.5490 - acc: 0.8750\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 263us/step - loss: 0.5450 - acc: 0.8750\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.5410 - acc: 0.8750\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.5370 - acc: 0.8750\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.5329 - acc: 0.8750\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.5289 - acc: 0.8750\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5247 - acc: 0.8750\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.5206 - acc: 0.8750\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5165 - acc: 0.8750\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5123 - acc: 0.8750\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5081 - acc: 0.8750\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.5039 - acc: 0.8750\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4997 - acc: 0.8750\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.4955 - acc: 0.8750\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.4912 - acc: 0.8750\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.4869 - acc: 0.8750\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4827 - acc: 0.8750\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.4784 - acc: 0.8750\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4741 - acc: 0.8750\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4698 - acc: 0.8750\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4655 - acc: 0.8750\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.4612 - acc: 0.9375\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4569 - acc: 0.9375\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4526 - acc: 0.9375\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4482 - acc: 0.9375\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4439 - acc: 0.9375\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4396 - acc: 0.9375\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4353 - acc: 0.9375\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4310 - acc: 0.9375\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.4267 - acc: 0.9375\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4224 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4181 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 247us/step - loss: 0.4138 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 247us/step - loss: 0.4095 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.4053 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.4010 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3968 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3925 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3883 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 312us/step - loss: 0.3841 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3800 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.3758 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3717 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.3675 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 248us/step - loss: 0.3634 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3594 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.3553 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3513 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.3473 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3433 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3393 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3354 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 220us/step - loss: 0.3315 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.3276 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 187us/step - loss: 0.3237 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3199 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 220us/step - loss: 0.3161 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a969351c8>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 17:30:00.769880 22736 base_any2vec.py:723] consider setting layer size to a multiple of 4 for greater performance\n",
      "W0529 17:30:01.599758 22736 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "D:\\programming python\\python_script\\NLP\\Natural_Language_processing_examples\\Basic_NLP\\fse\\models\\base_s2v.py:172: UserWarning: C extension not loaded, training/inferring will be slow. Install a C compiler and reinstall fse.\n",
      "  \"C extension not loaded, training/inferring will be slow. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4804146"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "ft = FastText(sentences, min_count=1, size=10)\n",
    "\n",
    "from fse.models import Average\n",
    "from fse import IndexedList\n",
    "model = Average(ft)\n",
    "model.train(IndexedList(sentences))\n",
    "\n",
    "model.sv.similarity(0,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
