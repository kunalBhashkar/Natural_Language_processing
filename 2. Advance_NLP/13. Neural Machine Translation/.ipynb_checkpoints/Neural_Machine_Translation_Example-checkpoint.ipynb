{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3aee62b9-47ce-e416-5816-8df7126fe690"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 16:35:08.744755  2368 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "654175ff-07c7-455d-1b8f-d702cee211c4"
   },
   "outputs": [],
   "source": [
    "frdata=[]\n",
    "endata=[]\n",
    "with open('data/train_fr_lines.txt',encoding='utf-8') as frfile:\n",
    "    for li in frfile:\n",
    "        frdata.append(li)\n",
    "with open('data/train_en_lines.txt',encoding='utf-8') as enfile:\n",
    "    for li in enfile:\n",
    "        endata.append(li)\n",
    "mtdata = pd.DataFrame({'FR':frdata,'EN':endata})\n",
    "mtdata['FR_len'] = mtdata['FR'].apply(lambda x: len(x.split(' ')))\n",
    "mtdata['EN_len'] = mtdata['EN'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voici Bill Lange. Je suis Dave Gallo.\\n'\n",
      " 'Nous allons vous raconter quelques histoires de la mer en vid√©o.\\n']\n",
      "[\"This is Bill Lange. I'm Dave Gallo.\\n\"\n",
      " \"And we're going to tell you some stories from the sea here in video.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(mtdata['FR'].head(2).values)\n",
    "print(mtdata['EN'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "48f10124-4c1b-1f09-512d-352c068de1b4"
   },
   "outputs": [],
   "source": [
    "mtdata_fr = []\n",
    "for fr in mtdata.FR:\n",
    "    mtdata_fr.append(fr)\n",
    "mtdata_en = []\n",
    "for en in mtdata.EN:\n",
    "    mtdata_en.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3f980247-3c32-240d-7d3d-7b0c3c6c13e5"
   },
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "3e9ce130-88f4-8779-5b5f-86f2a23ab347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total French words in Vocabulary: 159523\n",
      "Total English words in Vocabulary 127479\n"
     ]
    }
   ],
   "source": [
    "word_counts_dict_fr = {}\n",
    "word_counts_dict_en = {}\n",
    "count_words(word_counts_dict_fr, mtdata_fr)\n",
    "count_words(word_counts_dict_en, mtdata_en)\n",
    "            \n",
    "print(\"Total French words in Vocabulary:\", len(word_counts_dict_fr))\n",
    "print(\"Total English words in Vocabulary\", len(word_counts_dict_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sr = line.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            word = sr[0]\n",
    "            embedding = np.asarray(sr[1:], dtype='float32')\n",
    "            embedding_index[word] = embedding\n",
    "    return embedding_index\n",
    "embeddings_index = build_word_vector_matrix('./data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0be42a13-70b2-e9cc-7468-1247b01f109c"
   },
   "outputs": [],
   "source": [
    "def build_word2id_mapping(word_counts_dict):\n",
    "    word2int = {} \n",
    "    count_threshold = 20\n",
    "    value = 0\n",
    "    for word, count in word_counts_dict.items():\n",
    "        if count >= count_threshold or word in embeddings_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "\n",
    "\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "\n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    return word2int,int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(word2int):\n",
    "    embedding_dim = 50\n",
    "    nwords = len(word2int)\n",
    "\n",
    "    word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_emb_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    return word_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "4401990d-4baf-3f30-becc-3a6149716b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of french word embeddings:  19708\n",
      "Length of english word embeddings:  39614\n"
     ]
    }
   ],
   "source": [
    "fr_word2int,fr_int2word = build_word2id_mapping(word_counts_dict_fr)\n",
    "en_word2int,en_int2word = build_word2id_mapping(word_counts_dict_en)\n",
    "fr_embeddings_matrix = build_embeddings(fr_word2int)\n",
    "en_embeddings_matrix = build_embeddings(en_word2int)\n",
    "print(\"Length of french word embeddings: \", len(fr_embeddings_matrix))\n",
    "print(\"Length of english word embeddings: \", len(en_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "25cfd0e3-ae3d-8728-1c82-1a61bb06aa0e"
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, word2int, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "360cfdf4-ad4c-0316-56d3-70b6206e75e4"
   },
   "outputs": [],
   "source": [
    "id_fr, word_count_fr = convert_sentence_to_ids(mtdata_fr, fr_word2int)\n",
    "id_en, word_count_en = convert_sentence_to_ids(mtdata_en, en_word2int, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "2a0ae7cd-a845-23dc-3563-ad133e2f02b4"
   },
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence, word2int):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "50d631a2-fb5a-bb0d-6155-9cd15e70835b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered french/english sentences:  200404 200404\n"
     ]
    }
   ],
   "source": [
    "en_filtered = []\n",
    "fr_filtered = []\n",
    "max_en_length = int(mtdata.EN_len.max())\n",
    "max_fr_length = int(mtdata.FR_len.max())\n",
    "min_length = 4\n",
    "unknown_token_en_limit = 10\n",
    "unknown_token_fr_limit = 10\n",
    "\n",
    "for count,text in enumerate(id_en):\n",
    "    unknown_token_en = unknown_tokens(id_en[count],en_word2int)\n",
    "    unknown_token_fr = unknown_tokens(id_fr[count],fr_word2int)\n",
    "    en_len = len(id_en[count])\n",
    "    fr_len = len(id_fr[count])\n",
    "    if( (unknown_token_en>unknown_token_en_limit) or (unknown_token_fr>unknown_token_fr_limit) or \n",
    "       (en_len<min_length) or (fr_len<min_length) ):\n",
    "        continue\n",
    "    fr_filtered.append(id_fr[count])\n",
    "    en_filtered.append(id_en[count])\n",
    "print(\"Length of filtered french/english sentences: \", len(fr_filtered), len(en_filtered) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "34d28c5f-8016-6b36-664e-3d5ee3db745d"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs_data = tf.placeholder(tf.int32, [None, None], name='input_data')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_probs = tf.placeholder(tf.float32, name='dropout_probs')\n",
    "    en_len = tf.placeholder(tf.int32, (None,), name='en_len')\n",
    "    max_en_len = tf.reduce_max(en_len, name='max_en_len')\n",
    "    fr_len = tf.placeholder(tf.int32, (None,), name='fr_len')\n",
    "    return inputs_data, targets, learning_rate, dropout_probs, en_len, max_en_len, fr_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9c9b6087-3c28-478d-d311-4213e1c59654"
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "d675562b-a9e0-df71-6979-a052fb78dcbc"
   },
   "outputs": [],
   "source": [
    "def get_rnn_cell(rnn_cell_size,dropout_prob):\n",
    "    rnn_c = GRUCell(rnn_cell_size)\n",
    "    rnn_c = DropoutWrapper(rnn_c, input_keep_prob = dropout_prob)\n",
    "    return rnn_c\n",
    "\n",
    "def encoding_layer(rnn_cell_size, sequence_len, n_layers, rnn_inputs, dropout_prob):\n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            rnn_fw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            rnn_bw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            encoding_output, encoding_state = tf.nn.bidirectional_dynamic_rnn(rnn_fw, rnn_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_len,\n",
    "                                                                    dtype=tf.float32)\n",
    "    encoding_output = tf.concat(encoding_output,2)\n",
    "    return encoding_output, encoding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "524d0246-ddae-b485-5ea4-11ad476447f4"
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(decoding_embed_input, en_len, decoding_cell, initial_state, op_layer, \n",
    "                            v_size, max_en_len):\n",
    "    helper = TrainingHelper(inputs=decoding_embed_input,sequence_length=en_len, time_major=False)\n",
    "    dec = BasicDecoder(decoding_cell,helper,initial_state,op_layer) \n",
    "    logits, _, _ = dynamic_decode(dec,output_time_major=False,impute_finished=True, \n",
    "                                  maximum_iterations=max_en_len)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "6044b206-7f27-5304-4896-06d388af0949"
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, decoding_cell, initial_state, op_layer,\n",
    "                             max_en_len, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    inf_helper = GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "    inf_decoder = BasicDecoder(decoding_cell,inf_helper,initial_state,op_layer)       \n",
    "    inf_logits, _, _ = dynamic_decode(inf_decoder,output_time_major=False,impute_finished=True,\n",
    "                                                            maximum_iterations=max_en_len)\n",
    "    return inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "4b50746f-8f78-0253-9178-56c62e4ac1bf"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(decoding_embed_inp, embeddings, encoding_op, encoding_st, v_size, fr_len, \n",
    "                   en_len,max_en_len, rnn_cell_size, word2int, dropout_prob, batch_size, n_layers):\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('dec_rnn_layer_{}'.format(l)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_len)\n",
    "            decoding_cell = tf.contrib.rnn.DropoutWrapper(gru,input_keep_prob = dropout_prob)\n",
    "    out_l = Dense(v_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attention = BahdanauAttention(rnn_cell_size, encoding_op,fr_len,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    decoding_cell =  AttentionWrapper(decoding_cell,attention,rnn_len)\n",
    "    attention_zero_state = decoding_cell.zero_state(batch_size , tf.float32 )\n",
    "    attention_zero_state = attention_zero_state.clone(cell_state = encoding_st[0])\n",
    "    with tf.variable_scope(\"decoding_layer\"):\n",
    "        logits_tr = training_decoding_layer(decoding_embed_inp, \n",
    "                                                  en_len, \n",
    "                                                  decoding_cell, \n",
    "                                                  attention_zero_state,\n",
    "                                                  out_l,\n",
    "                                                  v_size, \n",
    "                                                  max_en_len)\n",
    "    with tf.variable_scope(\"decoding_layer\", reuse=True):\n",
    "        logits_inf = inference_decoding_layer(embeddings,  \n",
    "                                                    word2int[TOKEN_GO], \n",
    "                                                    word2int[TOKEN_EOS],\n",
    "                                                    decoding_cell, \n",
    "                                                    attention_zero_state, \n",
    "                                                    out_l,\n",
    "                                                    max_en_len,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return logits_tr, logits_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "19ddcf22-4f6a-d531-071a-021b42b643e3"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_en_data, dropout_prob, fr_len, en_len, max_en_len, \n",
    "                  v_size, rnn_cell_size, n_layers, word2int_en, batch_size):\n",
    "    \n",
    "    input_word_embeddings = tf.Variable(fr_embeddings_matrix, name=\"input_word_embeddings\")\n",
    "    encoding_embed_input = tf.nn.embedding_lookup(input_word_embeddings, input_data)\n",
    "    encoding_op, encoding_st = encoding_layer(rnn_cell_size, fr_len, n_layers, encoding_embed_input, dropout_prob)\n",
    "    \n",
    "    decoding_input = process_encoding_input(target_en_data, word2int_en, batch_size)\n",
    "    decoding_embed_input = tf.nn.embedding_lookup(en_embeddings_matrix, decoding_input)\n",
    "    \n",
    "    tr_logits, inf_logits  = decoding_layer(decoding_embed_input, \n",
    "                                                        en_embeddings_matrix,\n",
    "                                                        encoding_op,\n",
    "                                                        encoding_st, \n",
    "                                                        v_size, \n",
    "                                                        fr_len, \n",
    "                                                        en_len, \n",
    "                                                        max_en_len,\n",
    "                                                        rnn_cell_size, \n",
    "                                                        word2int_en, \n",
    "                                                        dropout_prob, \n",
    "                                                        batch_size,\n",
    "                                                        n_layers)\n",
    "    \n",
    "    return tr_logits, inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "725e92bf-2309-1a78-c771-641a42b440c6"
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_batch,word2int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentences_batch])\n",
    "    return [sentence + [word2int[TOKEN_PAD]] * (max_sentence - len(sentence)) for sentence in sentences_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "47e4f70a-6377-68dd-c06c-eed674b2bb3f"
   },
   "outputs": [],
   "source": [
    "def get_batches(en_text, fr_text, batch_size):\n",
    "    for batch_idx in range(0, len(fr_text)//batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        en_batch = en_text[start_idx:start_idx + batch_size]\n",
    "        fr_batch = fr_text[start_idx:start_idx + batch_size]\n",
    "        pad_en_batch = np.array(pad_sentences(en_batch, en_word2int))\n",
    "        pad_fr_batch = np.array(pad_sentences(fr_batch,fr_word2int))\n",
    "\n",
    "        pad_en_lens = []\n",
    "        for en_b in pad_en_batch:\n",
    "            pad_en_lens.append(len(en_b))\n",
    "        \n",
    "        pad_fr_lens = []\n",
    "        for fr_b in pad_fr_batch:\n",
    "            pad_fr_lens.append(len(fr_b))\n",
    "        \n",
    "        yield pad_en_batch, pad_fr_batch, pad_en_lens, pad_fr_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "77299c4b-a3cf-785b-981a-42a1bb3a2033"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "rnn_len = 256\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "dr_prob = 0.75\n",
    "logs_path='/tmp/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "68781626-8bf4-0a23-4bb2-f24a5762fa1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 16:35:31.964316  2368 deprecation.py:323] From <ipython-input-17-34d8a97edd7a>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0604 16:35:31.971300  2368 deprecation.py:323] From <ipython-input-17-34d8a97edd7a>:14: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0604 16:35:31.972295  2368 deprecation.py:323] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0604 16:35:32.102945  2368 deprecation.py:506] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0604 16:35:32.113916  2368 deprecation.py:506] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0604 16:35:32.133287  2368 deprecation.py:506] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0604 16:35:32.518027  2368 deprecation.py:323] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created.\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, targets, learning_rate, dropout_probs, en_len, max_en_len, fr_len = model_inputs()\n",
    "\n",
    "    logits_tr, logits_inf = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      dropout_probs,   \n",
    "                                                      fr_len,\n",
    "                                                      en_len,\n",
    "                                                      max_en_len,\n",
    "                                                      len(en_word2int)+1,\n",
    "                                                      rnn_len, \n",
    "                                                      n_layers, \n",
    "                                                      en_word2int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    logits_tr = tf.identity(logits_tr.rnn_output, 'logits_tr')\n",
    "    logits_inf = tf.identity(logits_inf.sample_id, name='predictions')\n",
    "    \n",
    "    seq_masks = tf.sequence_mask(en_len, max_en_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        tr_cost = sequence_loss(logits_tr,targets,seq_masks)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(tr_cost)\n",
    "        capped_gradients = [(tf.clip_by_value(gradient, -5., 5.), var) for gradient, var in gradients \n",
    "                        if gradient is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    tf.summary.scalar(\"cost\", tr_cost)\n",
    "print(\"Graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "6368ba0d-4083-e182-ca38-5356b307e09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch   1/20 Batch   20/3131 - Batch Loss:  6.573, seconds: 90.79\n",
      "** Epoch   1/20 Batch   40/3131 - Batch Loss:  2.394, seconds: 78.27\n",
      "** Epoch   1/20 Batch   60/3131 - Batch Loss:  2.405, seconds: 75.26\n",
      "** Epoch   1/20 Batch   80/3131 - Batch Loss:  2.044, seconds: 95.81\n",
      "** Epoch   1/20 Batch  100/3131 - Batch Loss:  2.157, seconds: 79.24\n",
      "** Epoch   1/20 Batch  120/3131 - Batch Loss:  2.205, seconds: 116.15\n",
      "** Epoch   1/20 Batch  140/3131 - Batch Loss:  2.393, seconds: 89.33\n",
      "Average loss: 2.813\n",
      "Saving model\n",
      "** Epoch   1/20 Batch  160/3131 - Batch Loss:  2.236, seconds: 88.43\n",
      "** Epoch   1/20 Batch  180/3131 - Batch Loss:  2.032, seconds: 74.79\n",
      "** Epoch   1/20 Batch  200/3131 - Batch Loss:  2.190, seconds: 102.04\n",
      "** Epoch   1/20 Batch  220/3131 - Batch Loss:  2.141, seconds: 56.06\n",
      "** Epoch   1/20 Batch  240/3131 - Batch Loss:  2.030, seconds: 87.56\n",
      "** Epoch   1/20 Batch  260/3131 - Batch Loss:  1.965, seconds: 208.28\n",
      "** Epoch   1/20 Batch  280/3131 - Batch Loss:  2.028, seconds: 76.22\n",
      "** Epoch   1/20 Batch  300/3131 - Batch Loss:  1.923, seconds: 79.80\n",
      "Average loss: 2.064\n",
      "Saving model\n",
      "** Epoch   1/20 Batch  320/3131 - Batch Loss:  1.994, seconds: 153.18\n",
      "** Epoch   1/20 Batch  340/3131 - Batch Loss:  2.100, seconds: 109.18\n",
      "** Epoch   1/20 Batch  360/3131 - Batch Loss:  2.015, seconds: 56.43\n",
      "** Epoch   1/20 Batch  380/3131 - Batch Loss:  1.888, seconds: 91.12\n",
      "** Epoch   1/20 Batch  400/3131 - Batch Loss:  1.902, seconds: 84.99\n",
      "** Epoch   1/20 Batch  420/3131 - Batch Loss:  2.026, seconds: 57.41\n",
      "** Epoch   1/20 Batch  440/3131 - Batch Loss:  1.821, seconds: 77.35\n",
      "** Epoch   1/20 Batch  460/3131 - Batch Loss:  1.973, seconds: 68.92\n",
      "Average loss: 1.948\n",
      "Saving model\n",
      "** Epoch   2/20 Batch   20/3131 - Batch Loss:  1.960, seconds: 114.30\n",
      "** Epoch   2/20 Batch   40/3131 - Batch Loss:  1.782, seconds: 139.06\n",
      "** Epoch   2/20 Batch   60/3131 - Batch Loss:  1.887, seconds: 84.97\n",
      "** Epoch   2/20 Batch   80/3131 - Batch Loss:  1.614, seconds: 136.41\n",
      "** Epoch   2/20 Batch  100/3131 - Batch Loss:  1.722, seconds: 105.38\n",
      "** Epoch   2/20 Batch  120/3131 - Batch Loss:  1.749, seconds: 123.59\n",
      "** Epoch   2/20 Batch  140/3131 - Batch Loss:  1.912, seconds: 91.27\n",
      "Average loss: 1.8\n",
      "Saving model\n",
      "** Epoch   2/20 Batch  160/3131 - Batch Loss:  1.805, seconds: 92.77\n",
      "** Epoch   2/20 Batch  180/3131 - Batch Loss:  1.627, seconds: 85.57\n",
      "** Epoch   2/20 Batch  200/3131 - Batch Loss:  1.760, seconds: 109.52\n",
      "** Epoch   2/20 Batch  220/3131 - Batch Loss:  1.731, seconds: 54.94\n",
      "** Epoch   2/20 Batch  240/3131 - Batch Loss:  1.655, seconds: 104.40\n",
      "** Epoch   2/20 Batch  260/3131 - Batch Loss:  1.637, seconds: 207.79\n",
      "** Epoch   2/20 Batch  280/3131 - Batch Loss:  1.679, seconds: 84.25\n",
      "** Epoch   2/20 Batch  300/3131 - Batch Loss:  1.565, seconds: 86.51\n",
      "Average loss: 1.681\n",
      "Saving model\n",
      "** Epoch   2/20 Batch  320/3131 - Batch Loss:  1.637, seconds: 156.18\n",
      "** Epoch   2/20 Batch  340/3131 - Batch Loss:  1.743, seconds: 133.55\n",
      "** Epoch   2/20 Batch  360/3131 - Batch Loss:  1.626, seconds: 64.95\n",
      "** Epoch   2/20 Batch  380/3131 - Batch Loss:  1.522, seconds: 107.56\n",
      "** Epoch   2/20 Batch  400/3131 - Batch Loss:  1.570, seconds: 92.51\n",
      "** Epoch   2/20 Batch  420/3131 - Batch Loss:  1.660, seconds: 73.32\n",
      "** Epoch   2/20 Batch  440/3131 - Batch Loss:  1.464, seconds: 83.14\n",
      "** Epoch   2/20 Batch  460/3131 - Batch Loss:  1.623, seconds: 74.05\n",
      "Average loss: 1.593\n",
      "Saving model\n",
      "** Epoch   3/20 Batch   20/3131 - Batch Loss:  1.648, seconds: 91.99\n",
      "** Epoch   3/20 Batch   40/3131 - Batch Loss:  1.480, seconds: 94.26\n",
      "** Epoch   3/20 Batch   60/3131 - Batch Loss:  1.613, seconds: 90.82\n",
      "** Epoch   3/20 Batch   80/3131 - Batch Loss:  1.372, seconds: 113.00\n",
      "** Epoch   3/20 Batch  100/3131 - Batch Loss:  1.431, seconds: 75.57\n",
      "** Epoch   3/20 Batch  120/3131 - Batch Loss:  1.432, seconds: 128.40\n",
      "** Epoch   3/20 Batch  140/3131 - Batch Loss:  1.610, seconds: 95.90\n",
      "Average loss: 1.51\n",
      "Saving model\n",
      "** Epoch   3/20 Batch  160/3131 - Batch Loss:  1.512, seconds: 92.06\n",
      "** Epoch   3/20 Batch  180/3131 - Batch Loss:  1.397, seconds: 79.08\n",
      "** Epoch   3/20 Batch  200/3131 - Batch Loss:  1.513, seconds: 114.38\n",
      "** Epoch   3/20 Batch  220/3131 - Batch Loss:  1.509, seconds: 56.54\n",
      "** Epoch   3/20 Batch  240/3131 - Batch Loss:  1.405, seconds: 100.73\n",
      "** Epoch   3/20 Batch  260/3131 - Batch Loss:  1.429, seconds: 192.01\n",
      "** Epoch   3/20 Batch  280/3131 - Batch Loss:  1.471, seconds: 80.18\n",
      "** Epoch   3/20 Batch  300/3131 - Batch Loss:  1.367, seconds: 85.98\n",
      "Average loss: 1.451\n",
      "Saving model\n",
      "** Epoch   3/20 Batch  320/3131 - Batch Loss:  1.390, seconds: 154.94\n",
      "** Epoch   3/20 Batch  340/3131 - Batch Loss:  1.507, seconds: 119.29\n",
      "** Epoch   3/20 Batch  360/3131 - Batch Loss:  1.409, seconds: 61.01\n",
      "** Epoch   3/20 Batch  380/3131 - Batch Loss:  1.296, seconds: 100.62\n",
      "** Epoch   3/20 Batch  400/3131 - Batch Loss:  1.356, seconds: 92.96\n",
      "** Epoch   3/20 Batch  420/3131 - Batch Loss:  1.458, seconds: 60.99\n",
      "** Epoch   3/20 Batch  440/3131 - Batch Loss:  1.248, seconds: 88.33\n",
      "** Epoch   3/20 Batch  460/3131 - Batch Loss:  1.434, seconds: 70.97\n",
      "Average loss: 1.38\n",
      "Saving model\n",
      "** Epoch   4/20 Batch   20/3131 - Batch Loss:  1.458, seconds: 127.24\n",
      "** Epoch   4/20 Batch   40/3131 - Batch Loss:  1.333, seconds: 95.44\n",
      "** Epoch   4/20 Batch   60/3131 - Batch Loss:  1.424, seconds: 97.44\n",
      "** Epoch   4/20 Batch   80/3131 - Batch Loss:  1.223, seconds: 142.39\n",
      "** Epoch   4/20 Batch  100/3131 - Batch Loss:  1.258, seconds: 116.59\n",
      "** Epoch   4/20 Batch  120/3131 - Batch Loss:  1.249, seconds: 168.58\n",
      "** Epoch   4/20 Batch  140/3131 - Batch Loss:  1.430, seconds: 125.23\n",
      "Average loss: 1.338\n",
      "Saving model\n",
      "** Epoch   4/20 Batch  160/3131 - Batch Loss:  1.342, seconds: 116.47\n",
      "** Epoch   4/20 Batch  180/3131 - Batch Loss:  1.216, seconds: 106.14\n",
      "** Epoch   4/20 Batch  200/3131 - Batch Loss:  1.327, seconds: 149.21\n",
      "** Epoch   4/20 Batch  220/3131 - Batch Loss:  1.327, seconds: 57.32\n",
      "** Epoch   4/20 Batch  240/3131 - Batch Loss:  1.255, seconds: 103.69\n",
      "** Epoch   4/20 Batch  260/3131 - Batch Loss:  1.295, seconds: 218.94\n",
      "** Epoch   4/20 Batch  280/3131 - Batch Loss:  1.311, seconds: 79.38\n",
      "** Epoch   4/20 Batch  300/3131 - Batch Loss:  1.197, seconds: 81.87\n",
      "Average loss: 1.28\n",
      "Saving model\n",
      "** Epoch   4/20 Batch  320/3131 - Batch Loss:  1.183, seconds: 187.57\n",
      "** Epoch   4/20 Batch  340/3131 - Batch Loss:  1.332, seconds: 120.17\n",
      "** Epoch   4/20 Batch  360/3131 - Batch Loss:  1.245, seconds: 85.15\n",
      "** Epoch   4/20 Batch  380/3131 - Batch Loss:  1.128, seconds: 142.37\n",
      "** Epoch   4/20 Batch  400/3131 - Batch Loss:  1.242, seconds: 96.43\n",
      "** Epoch   4/20 Batch  420/3131 - Batch Loss:  1.297, seconds: 70.49\n",
      "** Epoch   4/20 Batch  440/3131 - Batch Loss:  1.126, seconds: 125.10\n",
      "** Epoch   4/20 Batch  460/3131 - Batch Loss:  1.277, seconds: 106.11\n",
      "Average loss: 1.228\n",
      "Saving model\n",
      "** Epoch   5/20 Batch   20/3131 - Batch Loss:  1.304, seconds: 123.08\n",
      "** Epoch   5/20 Batch   40/3131 - Batch Loss:  1.168, seconds: 97.99\n",
      "** Epoch   5/20 Batch   60/3131 - Batch Loss:  1.306, seconds: 73.57\n",
      "** Epoch   5/20 Batch   80/3131 - Batch Loss:  1.107, seconds: 109.28\n",
      "** Epoch   5/20 Batch  100/3131 - Batch Loss:  1.120, seconds: 101.95\n",
      "** Epoch   5/20 Batch  120/3131 - Batch Loss:  1.126, seconds: 119.07\n",
      "** Epoch   5/20 Batch  140/3131 - Batch Loss:  1.290, seconds: 89.49\n",
      "Average loss: 1.203\n",
      "Saving model\n",
      "** Epoch   5/20 Batch  160/3131 - Batch Loss:  1.209, seconds: 83.64\n",
      "** Epoch   5/20 Batch  180/3131 - Batch Loss:  1.112, seconds: 69.20\n",
      "** Epoch   5/20 Batch  200/3131 - Batch Loss:  1.208, seconds: 125.48\n",
      "** Epoch   5/20 Batch  220/3131 - Batch Loss:  1.208, seconds: 65.05\n",
      "** Epoch   5/20 Batch  240/3131 - Batch Loss:  1.168, seconds: 88.46\n",
      "** Epoch   5/20 Batch  260/3131 - Batch Loss:  1.199, seconds: 212.39\n",
      "** Epoch   5/20 Batch  280/3131 - Batch Loss:  1.208, seconds: 89.87\n",
      "** Epoch   5/20 Batch  300/3131 - Batch Loss:  1.101, seconds: 92.34\n",
      "Average loss: 1.175\n",
      "Saving model\n",
      "** Epoch   5/20 Batch  320/3131 - Batch Loss:  1.088, seconds: 172.45\n",
      "** Epoch   5/20 Batch  340/3131 - Batch Loss:  1.229, seconds: 113.46\n",
      "** Epoch   5/20 Batch  360/3131 - Batch Loss:  1.163, seconds: 57.73\n",
      "** Epoch   5/20 Batch  380/3131 - Batch Loss:  1.061, seconds: 90.94\n",
      "** Epoch   5/20 Batch  400/3131 - Batch Loss:  1.147, seconds: 83.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch   5/20 Batch  420/3131 - Batch Loss:  1.187, seconds: 57.84\n",
      "** Epoch   5/20 Batch  440/3131 - Batch Loss:  1.047, seconds: 93.98\n",
      "** Epoch   5/20 Batch  460/3131 - Batch Loss:  1.194, seconds: 66.71\n",
      "Average loss: 1.14\n",
      "Saving model\n",
      "** Epoch   6/20 Batch   20/3131 - Batch Loss:  1.184, seconds: 88.81\n",
      "** Epoch   6/20 Batch   40/3131 - Batch Loss:  1.075, seconds: 74.98\n",
      "** Epoch   6/20 Batch   60/3131 - Batch Loss:  1.189, seconds: 71.58\n",
      "** Epoch   6/20 Batch   80/3131 - Batch Loss:  1.026, seconds: 97.18\n",
      "** Epoch   6/20 Batch  100/3131 - Batch Loss:  1.021, seconds: 70.59\n",
      "** Epoch   6/20 Batch  120/3131 - Batch Loss:  1.026, seconds: 109.19\n",
      "** Epoch   6/20 Batch  140/3131 - Batch Loss:  1.164, seconds: 94.00\n",
      "Average loss: 1.102\n",
      "Saving model\n",
      "** Epoch   6/20 Batch  160/3131 - Batch Loss:  1.139, seconds: 87.33\n",
      "** Epoch   6/20 Batch  180/3131 - Batch Loss:  1.071, seconds: 87.64\n",
      "** Epoch   6/20 Batch  200/3131 - Batch Loss:  1.168, seconds: 134.12\n",
      "** Epoch   6/20 Batch  220/3131 - Batch Loss:  1.151, seconds: 78.81\n",
      "** Epoch   6/20 Batch  240/3131 - Batch Loss:  1.116, seconds: 96.21\n",
      "** Epoch   6/20 Batch  260/3131 - Batch Loss:  1.122, seconds: 212.84\n",
      "** Epoch   6/20 Batch  280/3131 - Batch Loss:  1.141, seconds: 88.21\n",
      "** Epoch   6/20 Batch  300/3131 - Batch Loss:  1.035, seconds: 89.23\n",
      "Average loss: 1.115\n",
      "No Improvement.\n",
      "** Epoch   6/20 Batch  320/3131 - Batch Loss:  1.012, seconds: 148.23\n",
      "** Epoch   6/20 Batch  340/3131 - Batch Loss:  1.159, seconds: 107.47\n",
      "** Epoch   6/20 Batch  360/3131 - Batch Loss:  1.068, seconds: 65.07\n",
      "** Epoch   6/20 Batch  380/3131 - Batch Loss:  0.998, seconds: 88.16\n",
      "** Epoch   6/20 Batch  400/3131 - Batch Loss:  1.088, seconds: 80.97\n",
      "** Epoch   6/20 Batch  420/3131 - Batch Loss:  1.127, seconds: 54.29\n",
      "** Epoch   6/20 Batch  440/3131 - Batch Loss:  0.985, seconds: 78.02\n",
      "** Epoch   6/20 Batch  460/3131 - Batch Loss:  1.125, seconds: 66.58\n",
      "Average loss: 1.073\n",
      "Saving model\n",
      "** Epoch   7/20 Batch   20/3131 - Batch Loss:  1.146, seconds: 88.56\n",
      "** Epoch   7/20 Batch   40/3131 - Batch Loss:  1.029, seconds: 76.27\n",
      "** Epoch   7/20 Batch   60/3131 - Batch Loss:  1.124, seconds: 84.39\n",
      "** Epoch   7/20 Batch   80/3131 - Batch Loss:  0.974, seconds: 111.11\n",
      "** Epoch   7/20 Batch  100/3131 - Batch Loss:  0.979, seconds: 72.47\n",
      "** Epoch   7/20 Batch  120/3131 - Batch Loss:  0.977, seconds: 123.23\n",
      "** Epoch   7/20 Batch  140/3131 - Batch Loss:  1.098, seconds: 91.85\n",
      "Average loss: 1.05\n",
      "Saving model\n",
      "** Epoch   7/20 Batch  160/3131 - Batch Loss:  1.095, seconds: 104.01\n",
      "** Epoch   7/20 Batch  180/3131 - Batch Loss:  1.061, seconds: 86.68\n",
      "** Epoch   7/20 Batch  200/3131 - Batch Loss:  1.113, seconds: 110.51\n",
      "** Epoch   7/20 Batch  220/3131 - Batch Loss:  1.076, seconds: 60.53\n",
      "** Epoch   7/20 Batch  240/3131 - Batch Loss:  1.075, seconds: 96.06\n",
      "** Epoch   7/20 Batch  260/3131 - Batch Loss:  1.104, seconds: 193.49\n",
      "** Epoch   7/20 Batch  280/3131 - Batch Loss:  1.104, seconds: 81.91\n",
      "** Epoch   7/20 Batch  300/3131 - Batch Loss:  1.003, seconds: 84.89\n",
      "Average loss: 1.078\n",
      "No Improvement.\n",
      "** Epoch   7/20 Batch  320/3131 - Batch Loss:  0.977, seconds: 155.52\n",
      "** Epoch   7/20 Batch  340/3131 - Batch Loss:  1.133, seconds: 115.22\n",
      "** Epoch   7/20 Batch  360/3131 - Batch Loss:  1.035, seconds: 63.81\n",
      "** Epoch   7/20 Batch  380/3131 - Batch Loss:  0.941, seconds: 95.09\n",
      "** Epoch   7/20 Batch  400/3131 - Batch Loss:  1.036, seconds: 96.18\n",
      "** Epoch   7/20 Batch  420/3131 - Batch Loss:  1.088, seconds: 60.07\n",
      "** Epoch   7/20 Batch  440/3131 - Batch Loss:  0.921, seconds: 117.58\n",
      "** Epoch   7/20 Batch  460/3131 - Batch Loss:  1.062, seconds: 84.37\n",
      "Average loss: 1.026\n",
      "Saving model\n",
      "** Epoch   8/20 Batch   20/3131 - Batch Loss:  1.083, seconds: 98.87\n",
      "** Epoch   8/20 Batch   40/3131 - Batch Loss:  1.006, seconds: 78.28\n",
      "** Epoch   8/20 Batch   60/3131 - Batch Loss:  1.093, seconds: 75.98\n",
      "** Epoch   8/20 Batch   80/3131 - Batch Loss:  0.941, seconds: 100.71\n",
      "** Epoch   8/20 Batch  100/3131 - Batch Loss:  0.918, seconds: 78.12\n",
      "** Epoch   8/20 Batch  120/3131 - Batch Loss:  0.926, seconds: 127.66\n",
      "** Epoch   8/20 Batch  140/3131 - Batch Loss:  1.062, seconds: 101.71\n",
      "Average loss: 1.006\n",
      "Saving model\n",
      "** Epoch   8/20 Batch  160/3131 - Batch Loss:  1.034, seconds: 97.74\n",
      "** Epoch   8/20 Batch  180/3131 - Batch Loss:  0.989, seconds: 82.71\n",
      "** Epoch   8/20 Batch  200/3131 - Batch Loss:  1.054, seconds: 120.27\n",
      "** Epoch   8/20 Batch  220/3131 - Batch Loss:  1.010, seconds: 59.98\n",
      "** Epoch   8/20 Batch  240/3131 - Batch Loss:  0.983, seconds: 91.93\n",
      "** Epoch   8/20 Batch  260/3131 - Batch Loss:  1.025, seconds: 225.80\n",
      "** Epoch   8/20 Batch  280/3131 - Batch Loss:  1.028, seconds: 78.78\n",
      "** Epoch   8/20 Batch  300/3131 - Batch Loss:  0.947, seconds: 90.31\n",
      "Average loss: 1.007\n",
      "No Improvement.\n",
      "** Epoch   8/20 Batch  320/3131 - Batch Loss:  0.919, seconds: 175.26\n",
      "** Epoch   8/20 Batch  340/3131 - Batch Loss:  1.106, seconds: 115.28\n",
      "** Epoch   8/20 Batch  360/3131 - Batch Loss:  0.977, seconds: 61.12\n",
      "** Epoch   8/20 Batch  380/3131 - Batch Loss:  0.891, seconds: 95.68\n",
      "** Epoch   8/20 Batch  400/3131 - Batch Loss:  0.973, seconds: 83.87\n",
      "** Epoch   8/20 Batch  420/3131 - Batch Loss:  1.017, seconds: 57.92\n",
      "** Epoch   8/20 Batch  440/3131 - Batch Loss:  0.871, seconds: 96.35\n",
      "** Epoch   8/20 Batch  460/3131 - Batch Loss:  1.006, seconds: 71.08\n",
      "Average loss: 0.973\n",
      "Saving model\n",
      "** Epoch   9/20 Batch   20/3131 - Batch Loss:  1.024, seconds: 95.68\n",
      "** Epoch   9/20 Batch   40/3131 - Batch Loss:  0.940, seconds: 81.70\n",
      "** Epoch   9/20 Batch   60/3131 - Batch Loss:  1.049, seconds: 76.10\n",
      "** Epoch   9/20 Batch   80/3131 - Batch Loss:  0.910, seconds: 103.26\n",
      "** Epoch   9/20 Batch  100/3131 - Batch Loss:  0.888, seconds: 73.94\n",
      "** Epoch   9/20 Batch  120/3131 - Batch Loss:  0.887, seconds: 129.32\n",
      "** Epoch   9/20 Batch  140/3131 - Batch Loss:  1.004, seconds: 89.50\n",
      "Average loss: 0.96\n",
      "Saving model\n",
      "** Epoch   9/20 Batch  160/3131 - Batch Loss:  0.980, seconds: 81.44\n",
      "** Epoch   9/20 Batch  180/3131 - Batch Loss:  0.928, seconds: 69.50\n",
      "** Epoch   9/20 Batch  200/3131 - Batch Loss:  1.002, seconds: 104.94\n",
      "** Epoch   9/20 Batch  220/3131 - Batch Loss:  0.974, seconds: 62.96\n",
      "** Epoch   9/20 Batch  240/3131 - Batch Loss:  0.954, seconds: 101.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-244c57c0a5bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m                  \u001b[0men_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0men_text_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                  \u001b[0mfr_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfr_text_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                  dropout_probs: dr_prob})\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mupdate_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_learning_rate = 0.0006\n",
    "display_step = 20 \n",
    "stop_early_count = 0 \n",
    "stop_early_max_count = 3 \n",
    "per_epoch = 3 \n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] \n",
    "\n",
    "en_train = en_filtered[0:30000]\n",
    "fr_train = fr_filtered[0:30000]\n",
    "update_check = (len(fr_train)//batch_size//per_epoch)-1\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt' \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    tf_summary_writer = tf.summary.FileWriter(logs_path, graph=train_graph)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (en_batch, fr_batch, en_text_len, fr_text_len) in enumerate(\n",
    "                get_batches(en_train, fr_train, batch_size)):\n",
    "            before = time.time()\n",
    "            _,loss,summary = sess.run(\n",
    "                [train_op, tr_cost,merged_summary_op],\n",
    "                {input_data: fr_batch,\n",
    "                 targets: en_batch,\n",
    "                 learning_rate: lr,\n",
    "                 en_len: en_text_len,\n",
    "                 fr_len: fr_text_len,\n",
    "                 dropout_probs: dr_prob})\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            after = time.time()\n",
    "            batch_time = after - before\n",
    "            tf_summary_writer.add_summary(summary, epoch_i * batch_size + batch_i)\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('** Epoch {:>3}/{} Batch {:>4}/{} - Batch Loss: {:>6.3f}, seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(fr_filtered) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Saving model') \n",
    "                    stop_early_count = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early_count += 1\n",
    "                    if stop_early_count == stop_early_max_count:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "        if stop_early_count == stop_early_max_count:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "265fd2f2-cd5f-590d-1fa3-cf6eeedf89fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0604 21:49:52.885985  2368 deprecation.py:323] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "French Text\n",
      "  Word Ids:    [549, 1500, 413, 19704, 1907, 169, 17, 1479, 19704, 24, 19704, 131, 191, 21, 1908]\n",
      "  Input Words: Les syst√®mes tr√®s <UNK> partagent aussi des normes <UNK> et <UNK> √† tous les niveaux.\n",
      "\n",
      "English Text\n",
      "  Word Ids:       [34, 1813, 1486, 171, 171, 4536, 29, 2716, 77, 43, 1883, 21, 3831, 39612]\n",
      "  Response Words: The high-performing systems also also feeding and spaces, at all entire of other. <EOS>\n",
      " Ground Truth: <UNK> systems also share clear and ambitious standards across the entire spectrum. <EOS>\n"
     ]
    }
   ],
   "source": [
    "#random = np.random.randint(3000,len(fr_filtered))\n",
    "random = np.random.randint(0,3000)\n",
    "fr_text = fr_filtered[random]\n",
    "\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input_data:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    fr_length = loaded_graph.get_tensor_by_name('fr_len:0')\n",
    "    en_length = loaded_graph.get_tensor_by_name('en_len:0')\n",
    "    dropout_prob = loaded_graph.get_tensor_by_name('dropout_probs:0')\n",
    "    result_logits = sess.run(logits, {input_data: [fr_text]*batch_size, \n",
    "                                      en_length: [len(fr_text)], \n",
    "                                      fr_length: [len(fr_text)]*batch_size,\n",
    "                                      dropout_prob: 1.0})[0] \n",
    "\n",
    "pad = en_word2int[TOKEN_PAD] \n",
    "\n",
    "#print('\\nOriginal Text:', input_sentence)\n",
    "\n",
    "print('\\nFrench Text')\n",
    "print('  Word Ids:    {}'.format([i for i in fr_text]))\n",
    "print('  Input Words: {}'.format(\" \".join( [fr_int2word[i] for i in fr_text ] )))\n",
    "\n",
    "print('\\nEnglish Text')\n",
    "print('  Word Ids:       {}'.format([i for i in result_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join( [en_int2word[i]for i in result_logits if i!=pad] )))\n",
    "print(' Ground Truth: {}'.format(\" \".join( [en_int2word[i] for i in en_filtered[random]] )))"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
