{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.ibm1 import IBMModel1\n",
    "from nltk.translate.api import AlignedSent\n",
    "import dill as pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "def read_sents(filename):\n",
    "    sents = []\n",
    "    c=0\n",
    "    with open(filename,'r') as fi:\n",
    "        for li in fi:\n",
    "            sents.append(li.split())\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of english sentences:  5000\n",
      "Size of french sentences:  5000\n",
      "Training smt model\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "max_count=5000\n",
    "eng_sents_all = read_sents('data/train_en_lines.txt')\n",
    "fr_sents_all = read_sents('data/train_fr_lines.txt')\n",
    "eng_sents = eng_sents_all[:max_count]\n",
    "fr_sents = fr_sents_all[:max_count]\n",
    "print(\"Size of english sentences: \", len(eng_sents))\n",
    "print(\"Size of french sentences: \", len(fr_sents))\n",
    "aligned_text = []\n",
    "for i in range(len(eng_sents)):\n",
    "    al_sent = AlignedSent(fr_sents[i],eng_sents[i])\n",
    "    aligned_text.append(al_sent)\n",
    "print(\"Training smt model\")\n",
    "ibm_model = IBMModel1(aligned_text,5)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open('/tmp/ibm_smt.pkl','wb')\n",
    "pickle.dump(ibm_model,fi)\n",
    "fi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentence:  On appelle ça l'accessibilité financière.\n",
      "Translated Eng sentence:  suggests affordability. works. called called\n",
      "Original translation:  And it's called affordability.\n"
     ]
    }
   ],
   "source": [
    "#n_random = random.randint(max_count,len(fr_sents_all))\n",
    "n_random = random.randint(0,max_count)\n",
    "fr_sent = fr_sents_all[n_random]\n",
    "eng_sent_actual_tr = eng_sents_all[n_random]\n",
    "tr_sent = []\n",
    "for w in fr_sent:\n",
    "    probs = ibm_model.translation_table[w]\n",
    "    if(len(probs)==0):\n",
    "        continue\n",
    "    sorted_words = sorted([(k,v) for k, v in probs.items()],key=lambda x: x[1], reverse=True)\n",
    "    top_word = sorted_words[1][0]\n",
    "    if top_word is not None:\n",
    "        tr_sent.append(top_word)\n",
    "print(\"French sentence: \", \" \".join(fr_sent))\n",
    "print(\"Translated Eng sentence: \", \" \".join(tr_sent))\n",
    "print(\"Original translation: \", \" \".join(eng_sent_actual_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdata=[]\n",
    "endata=[]\n",
    "with open('data/train_fr_lines.txt') as frfile:\n",
    "    for li in frfile:\n",
    "        frdata.append(li)\n",
    "with open('data/train_en_lines.txt') as enfile:\n",
    "    for li in enfile:\n",
    "        endata.append(li)\n",
    "mtdata = pd.DataFrame({'FR':frdata,'EN':endata})\n",
    "mtdata['FR_len'] = mtdata['FR'].apply(lambda x: len(x.split(' ')))\n",
    "mtdata['EN_len'] = mtdata['EN'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtdata['FR'].head(2).values)\n",
    "print(mtdata['EN'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtdata_fr = []\n",
    "for fr in mtdata.FR:\n",
    "    mtdata_fr.append(fr)\n",
    "mtdata_en = []\n",
    "for en in mtdata.EN:\n",
    "    mtdata_en.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dict_fr = {}\n",
    "word_counts_dict_en = {}\n",
    "count_words(word_counts_dict_fr, mtdata_fr)\n",
    "count_words(word_counts_dict_en, mtdata_en)\n",
    "            \n",
    "print(\"Total French words in Vocabulary:\", len(word_counts_dict_fr))\n",
    "print(\"Total English words in Vocabulary\", len(word_counts_dict_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sr = line.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            word = sr[0]\n",
    "            embedding = np.asarray(sr[1:], dtype='float32')\n",
    "            embedding_index[word] = embedding\n",
    "    return embedding_index\n",
    "embeddings_index = build_word_vector_matrix('/Users/i346047/prs/temp/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2id_mapping(word_counts_dict):\n",
    "    word2int = {} \n",
    "    count_threshold = 20\n",
    "    value = 0\n",
    "    for word, count in word_counts_dict.items():\n",
    "        if count >= count_threshold or word in embeddings_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "\n",
    "\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "\n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    return word2int,int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(word2int):\n",
    "    embedding_dim = 50\n",
    "    nwords = len(word2int)\n",
    "\n",
    "    word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_emb_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    return word_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2int,fr_int2word = build_word2id_mapping(word_counts_dict_fr)\n",
    "en_word2int,en_int2word = build_word2id_mapping(word_counts_dict_en)\n",
    "fr_embeddings_matrix = build_embeddings(fr_word2int)\n",
    "en_embeddings_matrix = build_embeddings(en_word2int)\n",
    "print(\"Length of french word embeddings: \", len(fr_embeddings_matrix))\n",
    "print(\"Length of english word embeddings: \", len(en_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, word2int, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_fr, word_count_fr = convert_sentence_to_ids(mtdata_fr, fr_word2int)\n",
    "id_en, word_count_en = convert_sentence_to_ids(mtdata_en, en_word2int, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence, word2int):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_filtered = []\n",
    "fr_filtered = []\n",
    "max_en_length = int(mtdata.EN_len.max())\n",
    "max_fr_length = int(mtdata.FR_len.max())\n",
    "min_length = 4\n",
    "unknown_token_en_limit = 10\n",
    "unknown_token_fr_limit = 10\n",
    "\n",
    "for count,text in enumerate(id_en):\n",
    "    unknown_token_en = unknown_tokens(id_en[count],en_word2int)\n",
    "    unknown_token_fr = unknown_tokens(id_fr[count],fr_word2int)\n",
    "    en_len = len(id_en[count])\n",
    "    fr_len = len(id_fr[count])\n",
    "    if( (unknown_token_en>unknown_token_en_limit) or (unknown_token_fr>unknown_token_fr_limit) or \n",
    "       (en_len<min_length) or (fr_len<min_length) ):\n",
    "        continue\n",
    "    fr_filtered.append(id_fr[count])\n",
    "    en_filtered.append(id_en[count])\n",
    "print(\"Length of filtered french/english sentences: \", len(fr_filtered), len(en_filtered) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs_data = tf.placeholder(tf.int32, [None, None], name='input_data')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_probs = tf.placeholder(tf.float32, name='dropout_probs')\n",
    "    en_len = tf.placeholder(tf.int32, (None,), name='en_len')\n",
    "    max_en_len = tf.reduce_max(en_len, name='max_en_len')\n",
    "    fr_len = tf.placeholder(tf.int32, (None,), name='fr_len')\n",
    "    return inputs_data, targets, learning_rate, dropout_probs, en_len, max_en_len, fr_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cell(rnn_cell_size,dropout_prob):\n",
    "    rnn_c = GRUCell(rnn_cell_size)\n",
    "    rnn_c = DropoutWrapper(rnn_c, input_keep_prob = dropout_prob)\n",
    "    return rnn_c\n",
    "\n",
    "def encoding_layer(rnn_cell_size, sequence_len, n_layers, rnn_inputs, dropout_prob):\n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            rnn_fw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            rnn_bw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            encoding_output, encoding_state = tf.nn.bidirectional_dynamic_rnn(rnn_fw, rnn_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_len,\n",
    "                                                                    dtype=tf.float32)\n",
    "    encoding_output = tf.concat(encoding_output,2)\n",
    "    return encoding_output, encoding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
