{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from util import find_analogies\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rnn_class.util import get_wikipedia_data\n",
    "from rnn_class.brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Glove\n",
    "class Glove:\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print(\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    # i is not the word index!!!\n",
    "                    # j is not the word index!!!\n",
    "                    # i just points to which element of the sequence (sentence) we're looking at\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    # left side\n",
    "                    for j in range(start, i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "                    # right side\n",
    "                    for j in range(i + 1, end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (j - i) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "\n",
    "        print(\"max in X:\", X.max())\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print(\"max in f(X):\", fX.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print(\"max in log(X):\", logX.max())\n",
    "\n",
    "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in range(epochs):\n",
    "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logX\n",
    "            cost = ( fX * delta * delta ).sum()\n",
    "            costs.append(cost)\n",
    "            print(\"epoch:\", epoch, \"cost:\", cost)\n",
    "\n",
    "            if gd:\n",
    "                # gradient descent method\n",
    "                # update W\n",
    "                # oldW = W.copy()\n",
    "                for i in range(V):\n",
    "                    # for j in range(V):\n",
    "                    #     W[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*U[j]\n",
    "                    W[i] -= learning_rate*(fX[i,:]*delta[i,:]).dot(U)\n",
    "                W -= learning_rate*reg*W\n",
    "                # print \"updated W\"\n",
    "\n",
    "                # update b\n",
    "                for i in range(V):\n",
    "                    # for j in range(V):\n",
    "                    #     b[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
    "                    b[i] -= learning_rate*fX[i,:].dot(delta[i,:])\n",
    "                # b -= learning_rate*reg*b\n",
    "                # print \"updated b\"\n",
    "\n",
    "                # update U\n",
    "                for j in range(V):\n",
    "                    # for i in range(V):\n",
    "                    #     U[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*W[i]\n",
    "                    U[j] -= learning_rate*(fX[:,j]*delta[:,j]).dot(W)\n",
    "                U -= learning_rate*reg*U\n",
    "                # print \"updated U\"\n",
    "\n",
    "                # update c\n",
    "                for j in range(V):\n",
    "                    # for i in range(V):\n",
    "                    #     c[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
    "                    c[j] -= learning_rate*fX[:,j].dot(delta[:,j])\n",
    "                # c -= learning_rate*reg*c\n",
    "                # print \"updated c\"\n",
    "\n",
    "            else:\n",
    "                # ALS method\n",
    "\n",
    "                # update W\n",
    "                # fast way\n",
    "                # t0 = datetime.now()\n",
    "                for i in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(U[j], U[j]) for j in range(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-5)\n",
    "                    vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
    "                    W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"fast way took:\", (datetime.now() - t0)\n",
    "\n",
    "                # slow way\n",
    "                # t0 = datetime.now()\n",
    "                # for i in range(V):\n",
    "                #     matrix2 = reg*np.eye(D)\n",
    "                #     vector2 = 0\n",
    "                #     for j in range(V):\n",
    "                #         matrix2 += fX[i,j]*np.outer(U[j], U[j])\n",
    "                #         vector2 += fX[i,j]*(logX[i,j] - b[i] - c[j])*U[j]\n",
    "                # print \"slow way took:\", (datetime.now() - t0)\n",
    "\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-5)\n",
    "                    # assert(np.abs(vector - vector2).sum() < 1e-5)\n",
    "                    # W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated W\"\n",
    "\n",
    "                # update b\n",
    "                for i in range(V):\n",
    "                    denominator = fX[i,:].sum() + reg\n",
    "                    # assert(denominator > 0)\n",
    "                    numerator = fX[i,:].dot(logX[i,:] - W[i].dot(U.T) - c - mu)\n",
    "                    # for j in range(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - c[j])\n",
    "                    b[i] = numerator / denominator\n",
    "                # print \"updated b\"\n",
    "\n",
    "                # update U\n",
    "                for j in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(W[i], W[i]) for i in range(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[:,j]*W.T).dot(W)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-8)\n",
    "                    vector = (fX[:,j]*(logX[:,j] - b - c[j] - mu)).dot(W)\n",
    "                    # matrix = reg*np.eye(D)\n",
    "                    # vector = 0\n",
    "                    # for i in range(V):\n",
    "                    #     matrix += fX[i,j]*np.outer(W[i], W[i])\n",
    "                    #     vector += fX[i,j]*(logX[i,j] - b[i] - c[j])*W[i]\n",
    "                    U[j] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated U\"\n",
    "\n",
    "                # update c\n",
    "                for j in range(V):\n",
    "                    denominator = fX[:,j].sum() + reg\n",
    "                    numerator = fX[:,j].dot(logX[:,j] - W.dot(U[j]) - b  - mu)\n",
    "                    # for i in range(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - b[i])\n",
    "                    c[j] = numerator / denominator\n",
    "                # print \"updated c\"\n",
    "\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define main method\n",
    "def main(we_file, w2i_file, use_brown=True, n_files=100):\n",
    "    if use_brown:\n",
    "        cc_matrix = \"cc_matrix_brown.npy\"\n",
    "    else:\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
    "\n",
    "    # hacky way of checking if we need to re-load the raw data or not\n",
    "    # remember, only the co-occurrence matrix is needed for training\n",
    "    if os.path.exists(cc_matrix):\n",
    "        with open(w2i_file) as f:\n",
    "            word2idx = json.load(f)\n",
    "        sentences = [] # dummy - we won't actually use it\n",
    "    else:\n",
    "        if use_brown:\n",
    "            keep_words = set([\n",
    "                'king', 'man', 'woman',\n",
    "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "                'australia', 'australian', 'december', 'november', 'june',\n",
    "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "                'september', 'october',\n",
    "            ])\n",
    "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
    "        else:\n",
    "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
    "        \n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "\n",
    "    # alternating least squares method\n",
    "    model.fit(sentences, cc_matrix=cc_matrix, epochs=20)\n",
    "\n",
    "    # gradient descent method\n",
    "    # model.fit(\n",
    "    #     sentences,\n",
    "    #     cc_matrix=cc_matrix,\n",
    "    #     learning_rate=5e-4,\n",
    "    #     reg=0.1,\n",
    "    #     epochs=500,\n",
    "    #     gd=True,\n",
    "    # )\n",
    "    model.save(we_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you've downloaded, converted, and placed the Wikipedia data into the proper folder?\n",
      "I'm looking for a folder called large_files, adjacent to the class folder, but it does not exist.\n",
      "Please download the data from https://dumps.wikimedia.org/\n",
      "Quitting...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6267be4505f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# we = 'glove_model_brown.npz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# w2i = 'glove_word2idx_brown.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_brown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# load back embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0a0884bfc1c6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(we_file, w2i_file, use_brown, n_files)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences_with_word2idx_limit_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_wikipedia_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2i_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming python\\python_script\\NLP\\Natural_Language_processing_examples\\Advance_NLP\\2. Word2Vec\\rnn_class\\util.py\u001b[0m in \u001b[0;36mget_wikipedia_data\u001b[1;34m(n_files, n_vocab, by_paragraph)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please download the data from https://dumps.wikimedia.org/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Quitting...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0minput_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'enwiki'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == '__main__':\n",
    "    we = 'glove_model_50.npz'\n",
    "    w2i = 'glove_word2idx_50.json'\n",
    "    # we = 'glove_model_brown.npz'\n",
    "    # w2i = 'glove_word2idx_brown.json'\n",
    "    main(we, w2i, use_brown=False)\n",
    "    \n",
    "    # load back embeddings\n",
    "    npz = np.load(we)\n",
    "    W1 = npz['arr_0']\n",
    "    W2 = npz['arr_1']\n",
    "\n",
    "    with open(w2i) as f:\n",
    "        word2idx = json.load(f)\n",
    "        idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "    for concat in (True, False):\n",
    "        print(\"** concat:\", concat)\n",
    "\n",
    "        if concat:\n",
    "            We = np.hstack([W1, W2.T])\n",
    "        else:\n",
    "            We = (W1 + W2.T) / 2\n",
    "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from util import find_analogies\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rnn_class.util import get_wikipedia_data\n",
    "from rnn_class.brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx\n",
    "\n",
    "\n",
    "\n",
    "class Glove:\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print(\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    # i is not the word index!!!\n",
    "                    # j is not the word index!!!\n",
    "                    # i just points to which element of the sequence (sentence) we're looking at\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    # left side\n",
    "                    for j in range(start, i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "                    # right side\n",
    "                    for j in range(i + 1, end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (j - i) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "\n",
    "        print(\"max in X:\", X.max())\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print(\"max in f(X):\", fX.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print(\"max in log(X):\", logX.max())\n",
    "\n",
    "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "        # initialize weights, inputs, targets placeholders\n",
    "        tfW = tf.Variable(W.astype(np.float32))\n",
    "        tfb = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "        tfU = tf.Variable(U.astype(np.float32))\n",
    "        tfc = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
    "        tfLogX = tf.placeholder(tf.float32, shape=(V, V))\n",
    "        tffX = tf.placeholder(tf.float32, shape=(V, V))\n",
    "\n",
    "        delta = tf.matmul(tfW, tf.transpose(tfU)) + tfb + tfc + mu - tfLogX\n",
    "        cost = tf.reduce_sum(tffX * delta * delta)\n",
    "        regularized_cost = cost\n",
    "        for param in (tfW, tfU):\n",
    "            regularized_cost += reg*tf.reduce_sum(param * param)\n",
    "\n",
    "        train_op = tf.train.MomentumOptimizer(\n",
    "          learning_rate,\n",
    "          momentum=0.9\n",
    "        ).minimize(regularized_cost)\n",
    "        # train_op = tf.train.AdamOptimizer(1e-3).minimize(regularized_cost)\n",
    "        init = tf.global_variables_initializer()\n",
    "        session = tf.InteractiveSession()\n",
    "        session.run(init)\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in range(epochs):\n",
    "            c, _ = session.run((cost, train_op), feed_dict={tfLogX: logX, tffX: fX})\n",
    "            print(\"epoch:\", epoch, \"cost:\", c)\n",
    "            costs.append(c)\n",
    "\n",
    "        # save for future calculations\n",
    "        self.W, self.U = session.run([tfW, tfU])\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)\n",
    "\n",
    "\n",
    "def main(we_file, w2i_file, use_brown=True, n_files=50):\n",
    "    if use_brown:\n",
    "        cc_matrix = \"cc_matrix_brown.npy\"\n",
    "    else:\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
    "\n",
    "    # hacky way of checking if we need to re-load the raw data or not\n",
    "    # remember, only the co-occurrence matrix is needed for training\n",
    "    if os.path.exists(cc_matrix):\n",
    "        with open(w2i_file) as f:\n",
    "            word2idx = json.load(f)\n",
    "        sentences = [] # dummy - we won't actually use it\n",
    "    else:\n",
    "        if use_brown:\n",
    "            keep_words = set([\n",
    "                'king', 'man', 'woman',\n",
    "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "                'australia', 'australian', 'december', 'november', 'june',\n",
    "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "                'september', 'october',\n",
    "            ])\n",
    "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
    "        else:\n",
    "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
    "        \n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "    model.fit(sentences, cc_matrix=cc_matrix, epochs=200)\n",
    "    model.save(we_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    we = 'glove_model_50.npz'\n",
    "    w2i = 'glove_word2idx_50.json'\n",
    "    main(we, w2i, use_brown=False)\n",
    "\n",
    "    # load back embeddings\n",
    "    npz = np.load(we)\n",
    "    W1 = npz['arr_0']\n",
    "    W2 = npz['arr_1']\n",
    "\n",
    "    with open(w2i) as f:\n",
    "        word2idx = json.load(f)\n",
    "        idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "    for concat in (True, False):\n",
    "        print(\"** concat:\", concat)\n",
    "\n",
    "        if concat:\n",
    "            We = np.hstack([W1, W2.T])\n",
    "        else:\n",
    "            We = (W1 + W2.T) / 2\n",
    "\n",
    "\n",
    "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from util import find_analogies\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rnn_class.util import get_wikipedia_data\n",
    "from rnn_class.brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx\n",
    "\n",
    "\n",
    "class Glove:\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print(\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    # i is not the word index!!!\n",
    "                    # j is not the word index!!!\n",
    "                    # i just points to which element of the sequence (sentence) we're looking at\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    # left side\n",
    "                    for j in range(start, i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "                    # right side\n",
    "                    for j in range(i + 1, end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (j - i) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "\n",
    "        print(\"max in X:\", X.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print(\"max in log(X):\", logX.max())\n",
    "\n",
    "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # subtract global mean\n",
    "        mu = logX.mean()\n",
    "\n",
    "        model = TruncatedSVD(n_components=D)\n",
    "        Z = model.fit_transform(logX - mu)\n",
    "        S = np.diag(model.explained_variance_)\n",
    "        Sinv = np.linalg.inv(S)\n",
    "        self.W = Z.dot(Sinv)\n",
    "        self.U = model.components_.T\n",
    "\n",
    "        # calculate cost once\n",
    "        delta = self.W.dot(S).dot(self.U.T) + mu - logX\n",
    "        cost = (delta * delta).sum()\n",
    "        print(\"svd cost:\", cost)\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)\n",
    "\n",
    "\n",
    "def main(we_file, w2i_file, use_brown=True, n_files=100):\n",
    "    if use_brown:\n",
    "        cc_matrix = \"cc_matrix_brown.npy\"\n",
    "    else:\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
    "\n",
    "    # hacky way of checking if we need to re-load the raw data or not\n",
    "    # remember, only the co-occurrence matrix is needed for training\n",
    "    if os.path.exists(cc_matrix):\n",
    "        with open(w2i_file) as f:\n",
    "            word2idx = json.load(f)\n",
    "        sentences = [] # dummy - we won't actually use it\n",
    "    else:\n",
    "        if use_brown:\n",
    "            keep_words = set([\n",
    "                'king', 'man', 'woman',\n",
    "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "                'australia', 'australian', 'december', 'november', 'june',\n",
    "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "                'september', 'october',\n",
    "            ])\n",
    "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
    "        else:\n",
    "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
    "        \n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "\n",
    "    # alternating least squares method\n",
    "    model.fit(sentences, cc_matrix=cc_matrix)\n",
    "    model.save(we_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    we = 'glove_svd_50.npz'\n",
    "    w2i = 'glove_word2idx_50.json'\n",
    "    # we = 'glove_svd_brown.npz'\n",
    "    # w2i = 'glove_word2idx_brown.json'\n",
    "    main(we, w2i, use_brown=False)\n",
    "    \n",
    "    # load back embeddings\n",
    "    npz = np.load(we)\n",
    "    W1 = npz['arr_0']\n",
    "    W2 = npz['arr_1']\n",
    "\n",
    "    with open(w2i) as f:\n",
    "        word2idx = json.load(f)\n",
    "        idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "    for concat in (True, False):\n",
    "        print(\"** concat:\", concat)\n",
    "\n",
    "        if concat:\n",
    "            We = np.hstack([W1, W2.T])\n",
    "        else:\n",
    "            We = (W1 + W2.T) / 2\n",
    "\n",
    "\n",
    "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from util import find_analogies\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rnn_class.util import get_wikipedia_data\n",
    "from rnn_class.brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx\n",
    "\n",
    "\n",
    "def momentum_updates(cost, params, lr=1e-4, mu=0.9):\n",
    "  grads = T.grad(cost, params)\n",
    "  velocities = [theano.shared(\n",
    "    np.zeros_like(p.get_value()).astype(np.float32)\n",
    "  ) for p in params]\n",
    "  updates = []\n",
    "  for p, v, g in zip(params, velocities, grads):\n",
    "    newv = mu*v - lr*g\n",
    "    newp = p + newv\n",
    "    updates.append((p, newp))\n",
    "    updates.append((v, newv))\n",
    "  return updates\n",
    "\n",
    "\n",
    "class Glove:\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print(\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    # i is not the word index!!!\n",
    "                    # j is not the word index!!!\n",
    "                    # i just points to which element of the sequence (sentence) we're looking at\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    # left side\n",
    "                    for j in range(start, i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "                    # right side\n",
    "                    for j in range(i + 1, end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (j - i) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "\n",
    "        print(\"max in X:\", X.max())\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print(\"max in f(X):\", fX.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        # cast\n",
    "        fX = fX.astype(np.float32)\n",
    "        logX = logX.astype(np.float32)\n",
    "\n",
    "        print(\"max in log(X):\", logX.max())\n",
    "\n",
    "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "        # initialize weights, inputs, targets placeholders\n",
    "        thW = theano.shared(W.astype(np.float32))\n",
    "        thb = theano.shared(b.astype(np.float32))\n",
    "        thU = theano.shared(U.astype(np.float32))\n",
    "        thc = theano.shared(c.astype(np.float32))\n",
    "        thLogX = T.matrix('logX')\n",
    "        thfX = T.matrix('fX')\n",
    "\n",
    "        params = [thW, thb, thU, thc]\n",
    "\n",
    "        thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n",
    "        thCost = ( thfX * thDelta * thDelta ).sum()\n",
    "\n",
    "        # regularization\n",
    "        regularized_cost = thCost + reg*((thW * thW).sum() + (thU * thU).sum())\n",
    "\n",
    "        updates = momentum_updates(regularized_cost, params, learning_rate)\n",
    "\n",
    "        train_op = theano.function(\n",
    "            inputs=[thfX, thLogX],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in range(epochs):\n",
    "            train_op(fX, logX)\n",
    "            cost = cost_op(fX, logX)\n",
    "            costs.append(cost)\n",
    "            print(\"epoch:\", epoch, \"cost:\", cost)\n",
    "\n",
    "\n",
    "        self.W = thW.get_value()\n",
    "        self.U = thU.get_value()\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)\n",
    "\n",
    "\n",
    "def main(we_file, w2i_file, use_brown=True, n_files=50):\n",
    "    if use_brown:\n",
    "        cc_matrix = \"cc_matrix_brown.npy\"\n",
    "    else:\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
    "\n",
    "    # hacky way of checking if we need to re-load the raw data or not\n",
    "    # remember, only the co-occurrence matrix is needed for training\n",
    "    if os.path.exists(cc_matrix):\n",
    "        with open(w2i_file) as f:\n",
    "            word2idx = json.load(f)\n",
    "        sentences = [] # dummy - we won't actually use it\n",
    "    else:\n",
    "        if use_brown:\n",
    "            keep_words = set([\n",
    "                'king', 'man', 'woman',\n",
    "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "                'australia', 'australian', 'december', 'november', 'june',\n",
    "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "                'september', 'october',\n",
    "            ])\n",
    "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
    "        else:\n",
    "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
    "        \n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "    model.fit(\n",
    "        sentences,\n",
    "        cc_matrix=cc_matrix,\n",
    "        learning_rate=1e-4,\n",
    "        reg=0.1,\n",
    "        epochs=200,\n",
    "    )\n",
    "    model.save(we_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    we = 'glove_model_50.npz'\n",
    "    w2i = 'glove_word2idx_50.json'\n",
    "    # we = 'glove_model_brown.npz'\n",
    "    # w2i = 'glove_word2idx_brown.json'\n",
    "    main(we, w2i, use_brown=False)\n",
    "    \n",
    "    # load back embeddings\n",
    "    npz = np.load(we)\n",
    "    W1 = npz['arr_0']\n",
    "    W2 = npz['arr_1']\n",
    "\n",
    "    with open(w2i) as f:\n",
    "        word2idx = json.load(f)\n",
    "        idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "    for concat in (True, False):\n",
    "        print(\"** concat:\", concat)\n",
    "\n",
    "        if concat:\n",
    "            We = np.hstack([W1, W2.T])\n",
    "        else:\n",
    "            We = (W1 + W2.T) / 2\n",
    "\n",
    "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
