{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0603 21:21:49.729893 23768 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)  # vocab list\n",
    "\n",
    "# Parameter\n",
    "n_step = 5  # maxium number of words in one sentence(=number of time steps)\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods\n",
    "def make_batch(sentences):\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "# Model\n",
    "enc_inputs = tf.placeholder(tf.float32, [None, None, n_class])  # [batch_size, n_step, n_class]\n",
    "dec_inputs = tf.placeholder(tf.float32, [None, None, n_class])  # [batch_size, n_step, n_class]\n",
    "targets = tf.placeholder(tf.int64, [1, n_step])  # [batch_size, n_step], not one-hot\n",
    "\n",
    "# Linear for attention\n",
    "attn = tf.Variable(tf.random_normal([n_hidden, n_hidden]))\n",
    "out = tf.Variable(tf.random_normal([n_hidden * 2, n_class]))\n",
    "\n",
    "def get_att_score(dec_output, enc_output):  # enc_output [n_step, n_hidden]\n",
    "    score = tf.squeeze(tf.matmul(enc_output, attn), 0)  # score : [n_hidden]\n",
    "    dec_output = tf.squeeze(dec_output, [0, 1])  # dec_output : [n_hidden]\n",
    "    return tf.tensordot(dec_output, score, 1)  # inner product make scalar value\n",
    "\n",
    "def get_att_weight(dec_output, enc_outputs):\n",
    "    attn_scores = []  # list of attention scalar : [n_step]\n",
    "    enc_outputs = tf.transpose(enc_outputs, [1, 0, 2])  # enc_outputs : [n_step, batch_size, n_hidden]\n",
    "    for i in range(n_step):\n",
    "        attn_scores.append(get_att_score(dec_output, enc_outputs[i]))\n",
    "\n",
    "    # Normalize scores to weights in range 0 to 1\n",
    "    return tf.reshape(tf.nn.softmax(attn_scores), [1, 1, -1])  # [1, 1, n_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0603 21:23:01.495906 23768 deprecation.py:323] From <ipython-input-3-156ebb6d752d>:4: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0603 21:23:01.513925 23768 deprecation.py:323] From <ipython-input-3-156ebb6d752d>:8: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0603 21:23:01.598778 23768 deprecation.py:506] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0603 21:23:01.609748 23768 deprecation.py:506] From C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = []\n",
    "Attention = []\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    # enc_outputs : [batch_size(=1), n_step(=decoder_step), n_hidden(=128)]\n",
    "    # enc_hidden : [batch_size(=1), n_hidden(=128)]\n",
    "    enc_outputs, enc_hidden = tf.nn.dynamic_rnn(enc_cell, enc_inputs, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    inputs = tf.transpose(dec_inputs, [1, 0, 2])\n",
    "    hidden = enc_hidden\n",
    "    for i in range(n_step):\n",
    "        # time_major True mean inputs shape: [max_time, batch_size, ...]\n",
    "        dec_output, hidden = tf.nn.dynamic_rnn(dec_cell, tf.expand_dims(inputs[i], 1),\n",
    "                                               initial_state=hidden, dtype=tf.float32, time_major=True)\n",
    "        attn_weights = get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
    "        Attention.append(tf.squeeze(attn_weights))\n",
    "\n",
    "        # matrix-matrix product of matrices [1, 1, n_step] x [1, n_step, n_hidden] = [1, 1, n_hidden]\n",
    "        context = tf.matmul(attn_weights, enc_outputs)\n",
    "        dec_output = tf.squeeze(dec_output, 0)  # [1, n_step]\n",
    "        context = tf.squeeze(context, 1)  # [1, n_hidden]\n",
    "\n",
    "        model.append(tf.matmul(tf.concat((dec_output, context), 1), out))  # [n_step, batch_size(=1), n_class]\n",
    "\n",
    "trained_attn = tf.stack([Attention[0], Attention[1], Attention[2], Attention[3], Attention[4]], 0)  # to show attention matrix\n",
    "model = tf.transpose(model, [1, 0, 2])  # model : [n_step, n_class]\n",
    "prediction = tf.argmax(model, 2)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "Nt432JiJgboz",
    "outputId": "df4c9ae6-9df1-4a6d-f240-5123f887bd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 cost = 0.000001\n",
      "Epoch: 0800 cost = 0.000000\n",
      "Epoch: 1200 cost = 0.000000\n",
      "Epoch: 1600 cost = 0.000000\n",
      "Epoch: 2000 cost = 0.001581\n",
      "['ich', 'mochte', 'ein', 'bier', 'P'] -> ['i', 'want', 'a', 'beer', 'E']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training and Test\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(2000):\n",
    "        input_batch, output_batch, target_batch = make_batch(sentences)\n",
    "        _, loss, attention = sess.run([optimizer, cost, trained_attn],\n",
    "                                      feed_dict={enc_inputs: input_batch, dec_inputs: output_batch, targets: target_batch})\n",
    "\n",
    "        if (epoch + 1) % 400 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    predict_batch = [np.eye(n_class)[[word_dict[n] for n in 'P P P P P'.split()]]]\n",
    "    result = sess.run(prediction, feed_dict={enc_inputs: input_batch, dec_inputs: predict_batch})\n",
    "    print(sentences[0].split(), '->', [number_dict[n] for n in result[0]])\n",
    "\n",
    "    # Show Attention\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)  # vocab list\n",
    "\n",
    "# Parameter\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "def make_batch(sentences):\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "\n",
    "    # make tensor\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.Tensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "\n",
    "        # Linear for attention\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "\n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
    "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = Variable(torch.empty([n_step, 1, n_class]))\n",
    "\n",
    "        for i in range(n_step):  # each time step\n",
    "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "\n",
    "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
    "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "\n",
    "        # make model shape [n_step, n_class]\n",
    "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
    "\n",
    "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = Variable(torch.zeros(n_step))  # attn_scores : [n_step]\n",
    "\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "\n",
    "        # Normalize scores to weights in range 0 to 1\n",
    "        return F.softmax(attn_scores).view(1, 1, -1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
    "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "C:\\Users\\Kunal\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 cost = 0.000484\n",
      "Epoch: 0800 cost = 0.000152\n",
      "Epoch: 1200 cost = 0.000074\n",
      "Epoch: 1600 cost = 0.000043\n",
      "Epoch: 2000 cost = 0.000028\n",
      "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_batch, output_batch, target_batch = make_batch(sentences)\n",
    "\n",
    "# hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "hidden = Variable(torch.zeros(1, 1, n_hidden))\n",
    "\n",
    "model = Attention()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "    loss = criterion(output, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Test\n",
    "test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "test_batch = Variable(torch.Tensor(test_batch))\n",
    "predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "# Show Attention\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(trained_attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2Seq(Attention)-Tensor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
